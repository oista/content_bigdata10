{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--packages graphframes:graphframes:0.6.0-spark2.3-s_2.11 pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"/share/graphx\"\n",
    "graphPath = dataPath + \"/trainGraph\"\n",
    "usersToPredictPath = dataPath + \"/prediction.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.format(\"csv\").option(\"delimiter\", \"\\t\")\\\n",
    "        .load(graphPath).withColumnRenamed(\"_c0\", \"user\").withColumnRenamed(\"_c1\", \"friendsString\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "| user|       friendsString|\n",
      "+-----+--------------------+\n",
      "| 1424|{(846,0),(1691,25...|\n",
      "| 4128|{(49747,0),(53568...|\n",
      "| 4480|{(4677,0),(22256,...|\n",
      "| 4656|{(520,0),(12380,0...|\n",
      "| 5040|{(629,0),(2471,0)...|\n",
      "| 6288|{(24231,49152),(2...|\n",
      "| 9088|{(3887,32),(3921,...|\n",
      "|13360|{(58570,0),(74833...|\n",
      "|13568|{(852,0),(8729,0)...|\n",
      "|16480|{(10189,0),(10679...|\n",
      "|18048|{(24637,0),(24646...|\n",
      "|20464|{(7425,17408),(15...|\n",
      "|22256|{(4480,0),(9902,0...|\n",
      "|23424|{(1306,0),(8843,0...|\n",
      "|26320|{(5148,0),(5953,0...|\n",
      "|27744|{(12310,0),(25763...|\n",
      "|32128|{(26784,0),(39582...|\n",
      "|32352|{(8700,1024),(357...|\n",
      "|32896|{(9523,1),(12310,...|\n",
      "|35856|{(540,0),(3880,0)...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs, col, explode, collect_list, sort_array, size, split, lit\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def cutStartEndBrackets(s):\n",
    "    return s[2:-2]\n",
    "\n",
    "cutStartEndBracketsUDF = udf(cutStartEndBrackets, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "userFriend = data.select(col(\"user\"), split(cutStartEndBracketsUDF(col(\"friendsString\")), \"\\),\\(\").alias(\"friendsMasks\"))\\\n",
    "    .withColumn(\"friendMask\", explode('friendsMasks'))\\\n",
    "    .withColumn(\"friend\", split(col(\"friendMask\"), \",\")[0])\\\n",
    "    .select(col(\"user\").cast(\"integer\"), col(\"friend\").cast(\"integer\"))\n",
    "\n",
    "usersWithCommonFriend = userFriend\\\n",
    "    .groupBy(\"friend\")\\\n",
    "    .agg(sort_array(collect_list(\"user\")).alias(\"usersWithCommonFriend\"))\\\n",
    "    .where(size(col(\"usersWithCommonFriend\")) >= 2)\\\n",
    "    .select(col(\"usersWithCommonFriend\"))\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersToPredictDF = spark.read.format(\"csv\").option(\"delimiter\", \"\\t\")\\\n",
    "    .load(usersToPredictPath)\\\n",
    "    .withColumnRenamed(\"_c0\", \"user\")\n",
    "\n",
    "usersToPredict = set(int(user_row.user) for user_row in usersToPredictDF.collect())\n",
    "usersToPredictBC = sc.broadcast(usersToPredict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def pairsWithCommonFriend(usersWithCommonFriend):\n",
    "    pairs = []\n",
    "    for user1Index in range(0, len(usersWithCommonFriend) - 1):\n",
    "         for user2Index in range(user1Index + 1, len(usersWithCommonFriend)):\n",
    "            if user1Index in usersToPredictBC.value or user2Index in usersToPredictBC.value:\n",
    "                pairs.append((usersWithCommonFriend[user1Index], usersWithCommonFriend[user2Index]))\n",
    "    return pairs\n",
    "\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"user1\", IntegerType(), False),\n",
    "    StructField(\"user2\", IntegerType(), False)\n",
    "]))\n",
    "         \n",
    "pairsWithCommonFriendUdf = udf(pairsWithCommonFriend, schema)\n",
    "\n",
    "pairsPath = dataPath + \"/pairsUsersForPrediction\"\n",
    "\n",
    "usersWithCommonFriend\\\n",
    "        .select(pairsWithCommonFriendUdf(\"usersWithCommonFriend\").alias(\"pairsWithCommonFriend\"))\\\n",
    "        .where(size(col(\"pairsWithCommonFriend\")) > 0)\\\n",
    "        .write.mode(\"overwrite\").parquet(pairsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(pairsPath)\\\n",
    "    .withColumn(\"pairWithCommonFriend\", explode(\"pairsWithCommonFriend\"))\\\n",
    "    .drop(col(\"pairsWithCommonFriend\"))\\\n",
    "    .groupBy(col(\"pairWithCommonFriend\"))\\\n",
    "    .count()\\\n",
    "    .write.mode(\"overwrite\").parquet(dataPath + \"/pairsCount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Из-за чего переполняется память executor'а?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------+\n",
      "|usersWithCommonFriend|numberOfFriends|\n",
      "+---------------------+---------------+\n",
      "| [8952, 33884, 352...|           1559|\n",
      "| [37023, 57666, 10...|           1703|\n",
      "| [103433, 171851, ...|           1019|\n",
      "| [31996, 32873, 35...|           1274|\n",
      "| [31996, 48126, 50...|           1463|\n",
      "| [369, 29332, 3199...|           1533|\n",
      "| [31996, 35309, 87...|           1006|\n",
      "| [35309, 62113, 80...|           1132|\n",
      "| [74154, 135757, 1...|           1023|\n",
      "| [369, 14436, 2933...|           1094|\n",
      "| [49566, 50325, 54...|           1008|\n",
      "| [369, 107165, 148...|           1441|\n",
      "| [369, 2405, 29332...|           1001|\n",
      "| [369, 31996, 3530...|           1188|\n",
      "| [17669, 21647, 53...|           1161|\n",
      "| [35309, 60539, 11...|           1512|\n",
      "| [1583, 18068, 276...|           1411|\n",
      "| [41415, 91246, 18...|           1176|\n",
      "| [35309, 110726, 1...|           1143|\n",
      "| [66691, 68502, 72...|           1058|\n",
      "+---------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersWithCommonFriend \\\n",
    "    .withColumn(\"numberOfFriends\", size(col(\"usersWithCommonFriend\")))\\\n",
    "    .where(col(\"numberOfFriends\") >= 1000)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairsWithCommonFriendDivided(usersWithCommonFriend, divider):\n",
    "    pairs = []\n",
    "    for user1Index in range(0, len(usersWithCommonFriend) - 1):\n",
    "         for user2Index in range(user1Index + 1, len(usersWithCommonFriend)):\n",
    "                if user1Index % 17 == divider:\n",
    "                    pairs.append((usersWithCommonFriend[user1Index], usersWithCommonFriend[user2Index]))\n",
    "    return pairs\n",
    "\n",
    "schema = ArrayType(StructType([\n",
    "    StructField(\"user1\", IntegerType(), False),\n",
    "    StructField(\"user2\", IntegerType(), False)\n",
    "]))\n",
    "         \n",
    "pairsWithCommonFriendDividedUdf = udf(pairsWithCommonFriendDivided, schema)\n",
    "\n",
    "pairsPathPart = dataPath + \"/pairsPart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(17):\n",
    "    usersWithCommonFriend\\\n",
    "        .select(pairsWithCommonFriendDividedUdf(\"usersWithCommonFriend\", lit(i)).alias(\"pairsWithCommonFriend\"))\\\n",
    "        .where(size(col(\"pairsWithCommonFriend\")) > 0)\\\n",
    "        .withColumn(\"pairWithCommonFriend\", explode(\"pairsWithCommonFriend\"))\\\n",
    "        .drop(col(\"pairsWithCommonFriend\"))\\\n",
    "        .groupBy(col(\"pairWithCommonFriend\"))\\\n",
    "        .count()\\\n",
    "        .write.mode(\"overwrite\").parquet(pairsPathPart + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(pairsPathPart + \"*\")\\\n",
    "    .write.mode(\"overwrite\").parquet(dataPath + \"/pairsCount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
