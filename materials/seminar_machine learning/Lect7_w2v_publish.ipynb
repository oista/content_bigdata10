{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_file = zipfile.ZipFile('word2vec-nlp-tutorial.zip', 'r')\n",
    "zip_file.extractall()\n",
    "zip_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Файл labeledTrainData.tsv содержит тексты, которые размеченны по классам\n",
    "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Файл testData.tsv содержит тексты, по которым нужно выдать предсказания\n",
    "test = pd.read_csv('testData.tsv', header=0,  delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Обратим внимание на unlabeledTrainData.tsv\n",
    "# Для данного файла нет меток, к какому классу относятся его тексты\n",
    "# Так же организаторы не ждут предсказания для по классам для текстов из данного файла\n",
    "# В нем представленны тексты того же посола, что и остальные\n",
    "# А значит добавив его в обучение word2vec мы улучшим знание нашемй модели о \"мире\"\n",
    "unsup = pd.read_csv('unlabeledTrainData.tsv', header=0,  delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию которая для представленного текста:\n",
    "* меняет n't на not (по желанию)\n",
    "* приводит к нижнему регистру\n",
    "* делит на слова\n",
    "* удаляет стоп-слова (по желанию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    text = re.sub('n\\'t', ' not', text)\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Протестируем данную функцию\n",
    "\n",
    "text_to_wordlist(\"\"\"\n",
    "Tyrone Garland <s>(born 1992)</s> is an American professional basketball player \n",
    "who last played with the National <b>Basketball</b> League of Canada's Mississauga Power\n",
    "\"\"\")\n",
    "\n",
    "# Посмотрим что результатом будет list, элементы которого - слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию которая для представленного текста:\n",
    "* удаляет html теги\n",
    "* производит деления на предложения\n",
    "* каждое предложение делит на слова (применяя выше написанную функцию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "def text_to_sentences(text):\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(text_to_wordlist(raw_sentence))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Протестируем данную функцию\n",
    "\n",
    "text_to_sentences(\"\"\"\n",
    "Sherlock Holmes is a four-act play written by <p/> William Gillette and Sir Arthur Conan Doyle, \n",
    "based on Conan Doyle's eponymous character. It drew material from the stories \n",
    "<s>\"A Scandal in Bohemia\"</s>, \"The Final Problem\", and A Study in Scarlet, pitting Holmes \n",
    "against Professor Moriarty and reinventing the character of Irene Adler as a new love \n",
    "interest named Alice Faulkner. This play introduced the phrase \"Elementary, my dear Watson\" \n",
    "and Holmes' curved pipe.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Посмотрим что результатом будет list, элементы которого - list, элементы которого - слова\n",
    "# т.е. получаем list list'ов (список списков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так давайте соберем все имеющиеся тексты в подобню структуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sentences = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for review in unsup[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for review in test[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В конце сбора sentences будет list list'ов (список списков) - как и пример выше.\n",
    "# (Повторюсь) каждый элемент списка sentences - предложение, но представленное в виде списка слов - потому список\n",
    "\n",
    "# выведем количество элементов этого массива (оно же - количество предложений во всех текста)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а так же посмотрим на сам массив\n",
    "print ('\\t Первый элемент массива')\n",
    "print (sentences[0]) \n",
    "print ('\\t Второй элемент массива')\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим же теперь модель Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "\n",
    "# список параметров, которые можно менять по вашему желанию\n",
    "params = {}\n",
    "params['size'] = 300  # итоговая размерность вектора каждого слова\n",
    "params['min_count'] = 5  # минимальная частотность слова, чтобы оно попало в модель\n",
    "params['workers'] = 8     # количество ядер вашего процессора, чтоб запустить обучение в несколько потоков\n",
    "params['window'] = 10        # размер окна \n",
    "params['sample'] = 1e-3 # внутренняя метрика модели\n",
    "\n",
    "model = Word2Vec(sentences, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Финализируем нашу модель. Ее нельзя будет доучить теперь, но она начнет занимать гораздо меньше места\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренировав модель, получили представление каждого слова в семантическом пространстве (часто называют \"псевдо\" семантическое пространство)\n",
    "\n",
    "Попробуем популярный пример: QUEEN + MAN - KING = ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['queen', 'man'], negative=['king'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим слова, которые очень похожи на слово MOVIE\n",
    "\n",
    "model.most_similar('movie')\n",
    "\n",
    "# Попробуем и другие слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('mail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# так же взглянем на функцию doesnt_match, она покажет лишнее слово в массив\n",
    "\n",
    "model.doesnt_match(['berlin', 'rome', 'africa', 'london', 'paris'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(['man', 'woman', 'child', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# давайте просмотрим вектор одного из слов\n",
    "# его длину\n",
    "print (len(model['moon']))\n",
    "\n",
    "# и сам вектор\n",
    "print (model['moon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Словарь - все слова которые участвуют в модели можно просмотреть так\n",
    "len(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. мы хотим классифицировать не слова, а тексты, надо перевести тексты в вектора (представить в виде фич)\n",
    "Один из простых методов - сложить все вектора слов входящих в текст и поделить на число входящих слов.\n",
    "Напишем функцию, которая:\n",
    "* создает нулевой вектор - это будет результирующий вектор\n",
    "* идем по всем словам в тексте, если слово есть в моделе:\n",
    "  * увеличиваем счетчик слов\n",
    "  * прибавим вектор слова к результирующему вектору\n",
    "* поделим все координаты на число слов, вектора которых мы прибавляли к результирующему вектору"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vec(words, model, size):\n",
    "    text_vec = np.zeros((size,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "\n",
    "    index2word_set = set(model.wv.vocab.keys())\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words = n_words + 1\n",
    "            text_vec = np.add(text_vec, model[word])\n",
    "    \n",
    "    if n_words != 0:\n",
    "        text_vec /= n_words\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая на входе получает список всех текстов, а на выходе отдает список вектор каждого текста - что является прямоугольной матрицей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def texts_to_vecs(texts, model, size):\n",
    "    texts_vecs = np.zeros((len(texts), size), dtype=\"float32\")\n",
    "    \n",
    "    for i, text in tqdm(enumerate(texts),):\n",
    "        texts_vecs[i] = text_to_vec(text, model, size)\n",
    "\n",
    "    return texts_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим что функция texts_to_vecs принимает не просто тексты, а список всех слов текста. \n",
    "\n",
    "(!!!) Внимание: не список списков (там где сначала делили на предложения, а предложения на слова), а обычный линейный список\n",
    "\n",
    "Но у нас есть функции, которые переводят 1) текст в список предложений, 2) предложение в список слов\n",
    "\n",
    "Может показаться, что можно использовать 2ую функцию, но придется тогда ее переписать, потому как теги у нас удаляются лишь в первой функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поступим иначе, в python есть возможность развернуть двухмерный массив в одномерный, вот пример\n",
    "\n",
    "temp_list = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "print (sum(temp_list, []))\n",
    "\n",
    "# магия :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мы же такой возможностью воспользуемся, зная что функция text_to_sentences возвращает список списков\n",
    "\n",
    "list_of_list_of_words = text_to_sentences(\"\"\"\n",
    "Sherlock Holmes is a four-act play written by <p/> William Gillette and Sir Arthur Conan Doyle, \n",
    "based on Conan Doyle's eponymous character. It drew material from the stories \n",
    "<s>\"A Scandal in Bohemia\"</s>, \"The Final Problem\", and A Study in Scarlet, pitting Holmes \n",
    "against Professor Moriarty and reinventing the character of Irene Adler as a new love \n",
    "interest named Alice Faulkner. This play introduced the phrase \"Elementary, my dear Watson\" \n",
    "and Holmes' curved pipe.\n",
    "\"\"\")\n",
    "\n",
    "print ('\\t длина list_of_list_of_words')\n",
    "print (len(list_of_list_of_words))\n",
    "print ('\\t первый элемент list_of_list_of_words')\n",
    "print (list_of_list_of_words[0])\n",
    "\n",
    "list_of_words = sum(list_of_list_of_words, [])\n",
    "\n",
    "print ('\\t длина list_of_words')\n",
    "print (len(list_of_words))\n",
    "print ('\\t первый элемент list_of_words')\n",
    "print (list_of_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# действительно работает, сделаем для всех текстов из train\n",
    "train_like_word_list = [sum(text_to_sentences(text), []) for text in train['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = texts_to_vecs(train_like_word_list, model, params['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем тоже самое для test\n",
    "test_like_word_list = [sum(text_to_sentences(text), []) for text in test['review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs = texts_to_vecs(test_like_word_list, model, params['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Воспользуемся train_vecs, test_vecs, train[\"sentiment\"] \n",
    "#    как матрица фичей обучающей выборки, матрица фичей тестовой выборки, таргет для обучающей выборки соответственно\n",
    "\n",
    "# Стандартный случайный лес в таком случае\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs, train[\"sentiment\"])\n",
    "predict = forest.predict(test_vecs)\n",
    "\n",
    "# И вот задача решена"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
