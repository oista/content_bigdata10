{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_file = zipfile.ZipFile('word2vec-nlp-tutorial.zip', 'r')\n",
    "zip_file.extractall()\n",
    "zip_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/data/share/text7_ml_tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Файл labeledTrainData.tsv содержит тексты, которые размеченны по классам\n",
    "train = pd.read_csv('labeledTrainData.tsv', header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Файл testData.tsv содержит тексты, по которым нужно выдать предсказания\n",
    "test = pd.read_csv('testData.tsv', header=0,  delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "# Обратим внимание на unlabeledTrainData.tsv\n",
    "# Для данного файла нет меток, к какому классу относятся его тексты\n",
    "# Так же организаторы не ждут предсказания для по классам для текстов из данного файла\n",
    "# В нем представленны тексты того же посола, что и остальные\n",
    "# А значит добавив его в обучение word2vec мы улучшим знание нашемй модели о \"мире\"\n",
    "unsup = pd.read_csv('unlabeledTrainData.tsv', header=0,  delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию которая для представленного текста:\n",
    "* меняет n't на not (по желанию)\n",
    "* приводит к нижнему регистру\n",
    "* делит на слова\n",
    "* удаляет стоп-слова (по желанию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    text = re.sub('n\\'t', ' not', text)\n",
    "    \n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tyrone',\n",
       " 'garland',\n",
       " 's',\n",
       " 'born',\n",
       " 's',\n",
       " 'is',\n",
       " 'an',\n",
       " 'american',\n",
       " 'professional',\n",
       " 'basketball',\n",
       " 'player',\n",
       " 'who',\n",
       " 'last',\n",
       " 'played',\n",
       " 'with',\n",
       " 'the',\n",
       " 'national',\n",
       " 'b',\n",
       " 'basketball',\n",
       " 'b',\n",
       " 'league',\n",
       " 'of',\n",
       " 'canada',\n",
       " 's',\n",
       " 'mississauga',\n",
       " 'power']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Протестируем данную функцию\n",
    "\n",
    "text_to_wordlist(\"\"\"\n",
    "Tyrone Garland <s>(born 1992)</s> is an American professional basketball player \n",
    "who last played with the National <b>Basketball</b> League of Canada's Mississauga Power\n",
    "\"\"\")\n",
    "\n",
    "# Посмотрим что результатом будет list, элементы которого - слова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию которая для представленного текста:\n",
    "* удаляет html теги\n",
    "* производит деления на предложения\n",
    "* каждое предложение делит на слова (применяя выше написанную функцию)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "def text_to_sentences(text):\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(text_to_wordlist(raw_sentence))\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sherlock',\n",
       "  'holmes',\n",
       "  'is',\n",
       "  'a',\n",
       "  'four',\n",
       "  'act',\n",
       "  'play',\n",
       "  'written',\n",
       "  'by',\n",
       "  'william',\n",
       "  'gillette',\n",
       "  'and',\n",
       "  'sir',\n",
       "  'arthur',\n",
       "  'conan',\n",
       "  'doyle',\n",
       "  'based',\n",
       "  'on',\n",
       "  'conan',\n",
       "  'doyle',\n",
       "  's',\n",
       "  'eponymous',\n",
       "  'character'],\n",
       " ['it',\n",
       "  'drew',\n",
       "  'material',\n",
       "  'from',\n",
       "  'the',\n",
       "  'stories',\n",
       "  'a',\n",
       "  'scandal',\n",
       "  'in',\n",
       "  'bohemia',\n",
       "  'the',\n",
       "  'final',\n",
       "  'problem',\n",
       "  'and',\n",
       "  'a',\n",
       "  'study',\n",
       "  'in',\n",
       "  'scarlet',\n",
       "  'pitting',\n",
       "  'holmes',\n",
       "  'against',\n",
       "  'professor',\n",
       "  'moriarty',\n",
       "  'and',\n",
       "  'reinventing',\n",
       "  'the',\n",
       "  'character',\n",
       "  'of',\n",
       "  'irene',\n",
       "  'adler',\n",
       "  'as',\n",
       "  'a',\n",
       "  'new',\n",
       "  'love',\n",
       "  'interest',\n",
       "  'named',\n",
       "  'alice',\n",
       "  'faulkner'],\n",
       " ['this',\n",
       "  'play',\n",
       "  'introduced',\n",
       "  'the',\n",
       "  'phrase',\n",
       "  'elementary',\n",
       "  'my',\n",
       "  'dear',\n",
       "  'watson',\n",
       "  'and',\n",
       "  'holmes',\n",
       "  'curved',\n",
       "  'pipe']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Протестируем данную функцию\n",
    "\n",
    "text_to_sentences(\"\"\"\n",
    "Sherlock Holmes is a four-act play written by <p/> William Gillette and Sir Arthur Conan Doyle, \n",
    "based on Conan Doyle's eponymous character. It drew material from the stories \n",
    "<s>\"A Scandal in Bohemia\"</s>, \"The Final Problem\", and A Study in Scarlet, pitting Holmes \n",
    "against Professor Moriarty and reinventing the character of Irene Adler as a new love \n",
    "interest named Alice Faulkner. This play introduced the phrase \"Elementary, my dear Watson\" \n",
    "and Holmes' curved pipe.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Посмотрим что результатом будет list, элементы которого - list, элементы которого - слова\n",
    "# т.е. получаем list list'ов (список списков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так давайте соберем все имеющиеся тексты в подобню структуру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 s, sys: 1.29 s, total: 37.8 s\n",
      "Wall time: 38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentences = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 14s, sys: 2.67 s, total: 1min 17s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for review in unsup[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.4 s, sys: 1.33 s, total: 38.7 s\n",
      "Wall time: 39.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for review in test[\"review\"]:\n",
    "    sentences += text_to_sentences(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059231\n"
     ]
    }
   ],
   "source": [
    "# В конце сбора sentences будет list list'ов (список списков) - как и пример выше.\n",
    "# (Повторюсь) каждый элемент списка sentences - предложение, но представленное в виде списка слов - потому список\n",
    "\n",
    "# выведем количество элементов этого массива (оно же - количество предложений во всех текста)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Первый элемент массива\n",
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n",
      "\t Второй элемент массива\n",
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "# а так же посмотрим на сам массив\n",
    "print ('\\t Первый элемент массива')\n",
    "print (sentences[0]) \n",
    "print ('\\t Второй элемент массива')\n",
    "print (sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим же теперь модель Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "log = logging.getLogger()\n",
    "log.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 13:19:53,417 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-04-27 13:19:53,424 : INFO : collecting all words and their counts\n",
      "2019-04-27 13:19:53,425 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-27 13:19:53,519 : INFO : PROGRESS: at sentence #10000, processed 225522 words, keeping 17764 word types\n",
      "2019-04-27 13:19:53,606 : INFO : PROGRESS: at sentence #20000, processed 451018 words, keeping 24916 word types\n",
      "2019-04-27 13:19:53,702 : INFO : PROGRESS: at sentence #30000, processed 669878 words, keeping 30005 word types\n",
      "2019-04-27 13:19:53,803 : INFO : PROGRESS: at sentence #40000, processed 895595 words, keeping 34309 word types\n",
      "2019-04-27 13:19:53,879 : INFO : PROGRESS: at sentence #50000, processed 1114855 words, keeping 37727 word types\n",
      "2019-04-27 13:19:53,939 : INFO : PROGRESS: at sentence #60000, processed 1335610 words, keeping 40685 word types\n",
      "2019-04-27 13:19:54,009 : INFO : PROGRESS: at sentence #70000, processed 1557971 words, keeping 43279 word types\n",
      "2019-04-27 13:19:54,112 : INFO : PROGRESS: at sentence #80000, processed 1777121 words, keeping 45684 word types\n",
      "2019-04-27 13:19:54,195 : INFO : PROGRESS: at sentence #90000, processed 2000070 words, keeping 48084 word types\n",
      "2019-04-27 13:19:54,267 : INFO : PROGRESS: at sentence #100000, processed 2221535 words, keeping 50164 word types\n",
      "2019-04-27 13:19:54,381 : INFO : PROGRESS: at sentence #110000, processed 2440956 words, keeping 52031 word types\n",
      "2019-04-27 13:19:54,485 : INFO : PROGRESS: at sentence #120000, processed 2661763 words, keeping 54060 word types\n",
      "2019-04-27 13:19:54,564 : INFO : PROGRESS: at sentence #130000, processed 2886477 words, keeping 55787 word types\n",
      "2019-04-27 13:19:54,638 : INFO : PROGRESS: at sentence #140000, processed 3099495 words, keeping 57293 word types\n",
      "2019-04-27 13:19:54,735 : INFO : PROGRESS: at sentence #150000, processed 3324637 words, keeping 58987 word types\n",
      "2019-04-27 13:19:54,798 : INFO : PROGRESS: at sentence #160000, processed 3545968 words, keeping 60550 word types\n",
      "2019-04-27 13:19:54,865 : INFO : PROGRESS: at sentence #170000, processed 3768035 words, keeping 61976 word types\n",
      "2019-04-27 13:19:54,934 : INFO : PROGRESS: at sentence #180000, processed 3989111 words, keeping 63454 word types\n",
      "2019-04-27 13:19:55,000 : INFO : PROGRESS: at sentence #190000, processed 4214146 words, keeping 64741 word types\n",
      "2019-04-27 13:19:55,106 : INFO : PROGRESS: at sentence #200000, processed 4438275 words, keeping 66024 word types\n",
      "2019-04-27 13:19:55,199 : INFO : PROGRESS: at sentence #210000, processed 4659267 words, keeping 67316 word types\n",
      "2019-04-27 13:19:55,265 : INFO : PROGRESS: at sentence #220000, processed 4883502 words, keeping 68602 word types\n",
      "2019-04-27 13:19:55,333 : INFO : PROGRESS: at sentence #230000, processed 5105199 words, keeping 69874 word types\n",
      "2019-04-27 13:19:55,481 : INFO : PROGRESS: at sentence #240000, processed 5331838 words, keeping 71097 word types\n",
      "2019-04-27 13:19:55,600 : INFO : PROGRESS: at sentence #250000, processed 5546707 words, keeping 72276 word types\n",
      "2019-04-27 13:19:55,671 : INFO : PROGRESS: at sentence #260000, processed 5766043 words, keeping 73412 word types\n",
      "2019-04-27 13:19:55,778 : INFO : PROGRESS: at sentence #270000, processed 5984957 words, keeping 74671 word types\n",
      "2019-04-27 13:19:55,877 : INFO : PROGRESS: at sentence #280000, processed 6211305 words, keeping 76253 word types\n",
      "2019-04-27 13:19:55,977 : INFO : PROGRESS: at sentence #290000, processed 6433010 words, keeping 77724 word types\n",
      "2019-04-27 13:19:56,087 : INFO : PROGRESS: at sentence #300000, processed 6658547 words, keeping 79088 word types\n",
      "2019-04-27 13:19:56,178 : INFO : PROGRESS: at sentence #310000, processed 6882097 words, keeping 80385 word types\n",
      "2019-04-27 13:19:56,287 : INFO : PROGRESS: at sentence #320000, processed 7107674 words, keeping 81710 word types\n",
      "2019-04-27 13:19:56,399 : INFO : PROGRESS: at sentence #330000, processed 7329334 words, keeping 82945 word types\n",
      "2019-04-27 13:19:56,504 : INFO : PROGRESS: at sentence #340000, processed 7557957 words, keeping 84204 word types\n",
      "2019-04-27 13:19:56,611 : INFO : PROGRESS: at sentence #350000, processed 7780527 words, keeping 85325 word types\n",
      "2019-04-27 13:19:56,713 : INFO : PROGRESS: at sentence #360000, processed 8001478 words, keeping 86518 word types\n",
      "2019-04-27 13:19:56,825 : INFO : PROGRESS: at sentence #370000, processed 8227322 words, keeping 87588 word types\n",
      "2019-04-27 13:19:56,917 : INFO : PROGRESS: at sentence #380000, processed 8453371 words, keeping 88769 word types\n",
      "2019-04-27 13:19:57,012 : INFO : PROGRESS: at sentence #390000, processed 8681169 words, keeping 89819 word types\n",
      "2019-04-27 13:19:57,100 : INFO : PROGRESS: at sentence #400000, processed 8904720 words, keeping 90843 word types\n",
      "2019-04-27 13:19:57,185 : INFO : PROGRESS: at sentence #410000, processed 9125037 words, keeping 91796 word types\n",
      "2019-04-27 13:19:57,287 : INFO : PROGRESS: at sentence #420000, processed 9344994 words, keeping 92807 word types\n",
      "2019-04-27 13:19:57,380 : INFO : PROGRESS: at sentence #430000, processed 9572810 words, keeping 93852 word types\n",
      "2019-04-27 13:19:57,450 : INFO : PROGRESS: at sentence #440000, processed 9797425 words, keeping 94802 word types\n",
      "2019-04-27 13:19:57,546 : INFO : PROGRESS: at sentence #450000, processed 10022887 words, keeping 95939 word types\n",
      "2019-04-27 13:19:57,647 : INFO : PROGRESS: at sentence #460000, processed 10254604 words, keeping 97001 word types\n",
      "2019-04-27 13:19:57,714 : INFO : PROGRESS: at sentence #470000, processed 10481364 words, keeping 97831 word types\n",
      "2019-04-27 13:19:57,825 : INFO : PROGRESS: at sentence #480000, processed 10704505 words, keeping 98763 word types\n",
      "2019-04-27 13:19:57,941 : INFO : PROGRESS: at sentence #490000, processed 10927716 words, keeping 99790 word types\n",
      "2019-04-27 13:19:58,055 : INFO : PROGRESS: at sentence #500000, processed 11148936 words, keeping 100664 word types\n",
      "2019-04-27 13:19:58,180 : INFO : PROGRESS: at sentence #510000, processed 11372886 words, keeping 101607 word types\n",
      "2019-04-27 13:19:58,288 : INFO : PROGRESS: at sentence #520000, processed 11596590 words, keeping 102487 word types\n",
      "2019-04-27 13:19:58,357 : INFO : PROGRESS: at sentence #530000, processed 11823598 words, keeping 103329 word types\n",
      "2019-04-27 13:19:58,465 : INFO : PROGRESS: at sentence #540000, processed 12045373 words, keeping 104168 word types\n",
      "2019-04-27 13:19:58,563 : INFO : PROGRESS: at sentence #550000, processed 12268402 words, keeping 105050 word types\n",
      "2019-04-27 13:19:58,654 : INFO : PROGRESS: at sentence #560000, processed 12490514 words, keeping 105907 word types\n",
      "2019-04-27 13:19:58,755 : INFO : PROGRESS: at sentence #570000, processed 12717639 words, keeping 106692 word types\n",
      "2019-04-27 13:19:58,861 : INFO : PROGRESS: at sentence #580000, processed 12940620 words, keeping 107554 word types\n",
      "2019-04-27 13:19:58,960 : INFO : PROGRESS: at sentence #590000, processed 13164337 words, keeping 108374 word types\n",
      "2019-04-27 13:19:59,035 : INFO : PROGRESS: at sentence #600000, processed 13387776 words, keeping 109142 word types\n",
      "2019-04-27 13:19:59,103 : INFO : PROGRESS: at sentence #610000, processed 13609437 words, keeping 109995 word types\n",
      "2019-04-27 13:19:59,198 : INFO : PROGRESS: at sentence #620000, processed 13833835 words, keeping 110745 word types\n",
      "2019-04-27 13:19:59,302 : INFO : PROGRESS: at sentence #630000, processed 14056621 words, keeping 111504 word types\n",
      "2019-04-27 13:19:59,365 : INFO : PROGRESS: at sentence #640000, processed 14277366 words, keeping 112299 word types\n",
      "2019-04-27 13:19:59,474 : INFO : PROGRESS: at sentence #650000, processed 14502762 words, keeping 113087 word types\n",
      "2019-04-27 13:19:59,581 : INFO : PROGRESS: at sentence #660000, processed 14724550 words, keeping 113809 word types\n",
      "2019-04-27 13:19:59,689 : INFO : PROGRESS: at sentence #670000, processed 14949007 words, keeping 114546 word types\n",
      "2019-04-27 13:19:59,799 : INFO : PROGRESS: at sentence #680000, processed 15174075 words, keeping 115281 word types\n",
      "2019-04-27 13:19:59,907 : INFO : PROGRESS: at sentence #690000, processed 15394368 words, keeping 115972 word types\n",
      "2019-04-27 13:20:00,021 : INFO : PROGRESS: at sentence #700000, processed 15622344 words, keeping 116803 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 13:20:00,130 : INFO : PROGRESS: at sentence #710000, processed 15844652 words, keeping 117479 word types\n",
      "2019-04-27 13:20:00,237 : INFO : PROGRESS: at sentence #720000, processed 16071484 words, keeping 118118 word types\n",
      "2019-04-27 13:20:00,358 : INFO : PROGRESS: at sentence #730000, processed 16294148 words, keeping 118833 word types\n",
      "2019-04-27 13:20:00,430 : INFO : PROGRESS: at sentence #740000, processed 16516151 words, keeping 119544 word types\n",
      "2019-04-27 13:20:00,526 : INFO : PROGRESS: at sentence #750000, processed 16735891 words, keeping 120182 word types\n",
      "2019-04-27 13:20:00,590 : INFO : PROGRESS: at sentence #760000, processed 16953895 words, keeping 120821 word types\n",
      "2019-04-27 13:20:00,657 : INFO : PROGRESS: at sentence #770000, processed 17179782 words, keeping 121573 word types\n",
      "2019-04-27 13:20:00,738 : INFO : PROGRESS: at sentence #780000, processed 17408560 words, keeping 122302 word types\n",
      "2019-04-27 13:20:00,826 : INFO : PROGRESS: at sentence #790000, processed 17635306 words, keeping 122949 word types\n",
      "2019-04-27 13:20:00,897 : INFO : PROGRESS: at sentence #800000, processed 17858714 words, keeping 123771 word types\n",
      "2019-04-27 13:20:00,998 : INFO : PROGRESS: at sentence #810000, processed 18086202 words, keeping 124734 word types\n",
      "2019-04-27 13:20:01,097 : INFO : PROGRESS: at sentence #820000, processed 18305221 words, keeping 125613 word types\n",
      "2019-04-27 13:20:01,192 : INFO : PROGRESS: at sentence #830000, processed 18526553 words, keeping 126501 word types\n",
      "2019-04-27 13:20:01,296 : INFO : PROGRESS: at sentence #840000, processed 18744107 words, keeping 127235 word types\n",
      "2019-04-27 13:20:01,390 : INFO : PROGRESS: at sentence #850000, processed 18966774 words, keeping 127969 word types\n",
      "2019-04-27 13:20:01,487 : INFO : PROGRESS: at sentence #860000, processed 19189392 words, keeping 128773 word types\n",
      "2019-04-27 13:20:01,594 : INFO : PROGRESS: at sentence #870000, processed 19411480 words, keeping 129500 word types\n",
      "2019-04-27 13:20:01,698 : INFO : PROGRESS: at sentence #880000, processed 19628556 words, keeping 130162 word types\n",
      "2019-04-27 13:20:01,765 : INFO : PROGRESS: at sentence #890000, processed 19849352 words, keeping 130888 word types\n",
      "2019-04-27 13:20:01,859 : INFO : PROGRESS: at sentence #900000, processed 20063183 words, keeping 131550 word types\n",
      "2019-04-27 13:20:01,933 : INFO : PROGRESS: at sentence #910000, processed 20285399 words, keeping 132139 word types\n",
      "2019-04-27 13:20:02,025 : INFO : PROGRESS: at sentence #920000, processed 20508576 words, keeping 132768 word types\n",
      "2019-04-27 13:20:02,133 : INFO : PROGRESS: at sentence #930000, processed 20729500 words, keeping 133466 word types\n",
      "2019-04-27 13:20:02,231 : INFO : PROGRESS: at sentence #940000, processed 20952800 words, keeping 134111 word types\n",
      "2019-04-27 13:20:02,297 : INFO : PROGRESS: at sentence #950000, processed 21175018 words, keeping 134793 word types\n",
      "2019-04-27 13:20:02,403 : INFO : PROGRESS: at sentence #960000, processed 21398403 words, keeping 135431 word types\n",
      "2019-04-27 13:20:02,485 : INFO : PROGRESS: at sentence #970000, processed 21617117 words, keeping 136076 word types\n",
      "2019-04-27 13:20:02,582 : INFO : PROGRESS: at sentence #980000, processed 21833271 words, keeping 136614 word types\n",
      "2019-04-27 13:20:02,675 : INFO : PROGRESS: at sentence #990000, processed 22053890 words, keeping 137277 word types\n",
      "2019-04-27 13:20:02,787 : INFO : PROGRESS: at sentence #1000000, processed 22275984 words, keeping 137893 word types\n",
      "2019-04-27 13:20:02,892 : INFO : PROGRESS: at sentence #1010000, processed 22491076 words, keeping 138478 word types\n",
      "2019-04-27 13:20:02,993 : INFO : PROGRESS: at sentence #1020000, processed 22714741 words, keeping 139149 word types\n",
      "2019-04-27 13:20:03,108 : INFO : PROGRESS: at sentence #1030000, processed 22934414 words, keeping 139695 word types\n",
      "2019-04-27 13:20:03,201 : INFO : PROGRESS: at sentence #1040000, processed 23154408 words, keeping 140297 word types\n",
      "2019-04-27 13:20:03,309 : INFO : PROGRESS: at sentence #1050000, processed 23374861 words, keeping 140889 word types\n",
      "2019-04-27 13:20:03,396 : INFO : collected 141383 word types from a corpus of 23584060 raw words and 1059231 sentences\n",
      "2019-04-27 13:20:03,397 : INFO : Loading a fresh vocabulary\n",
      "2019-04-27 13:20:03,563 : INFO : min_count=5 retains 53172 unique words (37% of original 141383, drops 88211)\n",
      "2019-04-27 13:20:03,564 : INFO : min_count=5 leaves 23444438 word corpus (99% of original 23584060, drops 139622)\n",
      "2019-04-27 13:20:03,740 : INFO : deleting the raw counts dictionary of 141383 items\n",
      "2019-04-27 13:20:03,745 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-04-27 13:20:03,746 : INFO : downsampling leaves estimated 17475256 word corpus (74.5% of prior 23444438)\n",
      "2019-04-27 13:20:04,049 : INFO : estimated required memory for 53172 words and 300 dimensions: 154198800 bytes\n",
      "2019-04-27 13:20:04,050 : INFO : resetting layer weights\n",
      "2019-04-27 13:20:05,273 : INFO : training model with 8 workers on 53172 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-27 13:20:06,284 : INFO : EPOCH 1 - PROGRESS: at 4.34% examples, 761502 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:07,290 : INFO : EPOCH 1 - PROGRESS: at 9.13% examples, 792937 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:08,291 : INFO : EPOCH 1 - PROGRESS: at 13.61% examples, 787181 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:09,292 : INFO : EPOCH 1 - PROGRESS: at 17.89% examples, 776937 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:10,295 : INFO : EPOCH 1 - PROGRESS: at 21.90% examples, 761659 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:11,301 : INFO : EPOCH 1 - PROGRESS: at 26.77% examples, 774371 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:12,334 : INFO : EPOCH 1 - PROGRESS: at 30.63% examples, 757467 words/s, in_qsize 16, out_qsize 0\n",
      "2019-04-27 13:20:13,334 : INFO : EPOCH 1 - PROGRESS: at 34.75% examples, 753322 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:14,342 : INFO : EPOCH 1 - PROGRESS: at 38.31% examples, 739003 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:15,363 : INFO : EPOCH 1 - PROGRESS: at 42.15% examples, 730864 words/s, in_qsize 16, out_qsize 1\n",
      "2019-04-27 13:20:16,372 : INFO : EPOCH 1 - PROGRESS: at 45.68% examples, 721072 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:17,388 : INFO : EPOCH 1 - PROGRESS: at 48.97% examples, 708304 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:18,389 : INFO : EPOCH 1 - PROGRESS: at 52.17% examples, 697101 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:19,408 : INFO : EPOCH 1 - PROGRESS: at 55.67% examples, 690349 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:20,423 : INFO : EPOCH 1 - PROGRESS: at 58.99% examples, 682671 words/s, in_qsize 13, out_qsize 2\n",
      "2019-04-27 13:20:21,431 : INFO : EPOCH 1 - PROGRESS: at 62.47% examples, 677649 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:22,431 : INFO : EPOCH 1 - PROGRESS: at 66.13% examples, 675699 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:23,456 : INFO : EPOCH 1 - PROGRESS: at 68.90% examples, 664426 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:24,474 : INFO : EPOCH 1 - PROGRESS: at 72.36% examples, 660416 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:25,508 : INFO : EPOCH 1 - PROGRESS: at 75.74% examples, 656249 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:26,518 : INFO : EPOCH 1 - PROGRESS: at 79.19% examples, 653204 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:27,532 : INFO : EPOCH 1 - PROGRESS: at 82.71% examples, 651005 words/s, in_qsize 16, out_qsize 1\n",
      "2019-04-27 13:20:28,542 : INFO : EPOCH 1 - PROGRESS: at 86.03% examples, 647189 words/s, in_qsize 15, out_qsize 2\n",
      "2019-04-27 13:20:29,555 : INFO : EPOCH 1 - PROGRESS: at 89.26% examples, 643451 words/s, in_qsize 15, out_qsize 3\n",
      "2019-04-27 13:20:30,553 : INFO : EPOCH 1 - PROGRESS: at 92.64% examples, 640992 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:31,591 : INFO : EPOCH 1 - PROGRESS: at 96.07% examples, 638195 words/s, in_qsize 11, out_qsize 4\n",
      "2019-04-27 13:20:32,601 : INFO : EPOCH 1 - PROGRESS: at 99.73% examples, 637840 words/s, in_qsize 7, out_qsize 1\n",
      "2019-04-27 13:20:32,602 : INFO : worker thread finished; awaiting finish of 7 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 13:20:32,611 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-04-27 13:20:32,616 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-04-27 13:20:32,631 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-04-27 13:20:32,633 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-27 13:20:32,635 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 13:20:32,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 13:20:32,676 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 13:20:32,678 : INFO : EPOCH - 1 : training on 23584060 raw words (17473737 effective words) took 27.4s, 637842 effective words/s\n",
      "2019-04-27 13:20:33,725 : INFO : EPOCH 2 - PROGRESS: at 2.88% examples, 499143 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:34,737 : INFO : EPOCH 2 - PROGRESS: at 6.10% examples, 527196 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:35,757 : INFO : EPOCH 2 - PROGRESS: at 9.21% examples, 528139 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:36,763 : INFO : EPOCH 2 - PROGRESS: at 13.28% examples, 570412 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:37,765 : INFO : EPOCH 2 - PROGRESS: at 17.26% examples, 594972 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:38,775 : INFO : EPOCH 2 - PROGRESS: at 20.97% examples, 603175 words/s, in_qsize 13, out_qsize 2\n",
      "2019-04-27 13:20:39,779 : INFO : EPOCH 2 - PROGRESS: at 24.95% examples, 614798 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:40,780 : INFO : EPOCH 2 - PROGRESS: at 28.83% examples, 622813 words/s, in_qsize 16, out_qsize 1\n",
      "2019-04-27 13:20:41,787 : INFO : EPOCH 2 - PROGRESS: at 32.80% examples, 631262 words/s, in_qsize 16, out_qsize 0\n",
      "2019-04-27 13:20:42,787 : INFO : EPOCH 2 - PROGRESS: at 36.63% examples, 635416 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:43,791 : INFO : EPOCH 2 - PROGRESS: at 40.76% examples, 643289 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:44,805 : INFO : EPOCH 2 - PROGRESS: at 44.50% examples, 644351 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:45,825 : INFO : EPOCH 2 - PROGRESS: at 48.42% examples, 646745 words/s, in_qsize 12, out_qsize 3\n",
      "2019-04-27 13:20:46,828 : INFO : EPOCH 2 - PROGRESS: at 52.13% examples, 646938 words/s, in_qsize 13, out_qsize 2\n",
      "2019-04-27 13:20:47,840 : INFO : EPOCH 2 - PROGRESS: at 56.17% examples, 650629 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:48,856 : INFO : EPOCH 2 - PROGRESS: at 60.41% examples, 655481 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:49,878 : INFO : EPOCH 2 - PROGRESS: at 64.28% examples, 656129 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:50,885 : INFO : EPOCH 2 - PROGRESS: at 68.01% examples, 655992 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:51,886 : INFO : EPOCH 2 - PROGRESS: at 71.29% examples, 651433 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:52,889 : INFO : EPOCH 2 - PROGRESS: at 75.36% examples, 654617 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:53,898 : INFO : EPOCH 2 - PROGRESS: at 79.14% examples, 654478 words/s, in_qsize 10, out_qsize 5\n",
      "2019-04-27 13:20:54,919 : INFO : EPOCH 2 - PROGRESS: at 82.84% examples, 653364 words/s, in_qsize 15, out_qsize 2\n",
      "2019-04-27 13:20:55,926 : INFO : EPOCH 2 - PROGRESS: at 87.03% examples, 656234 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:56,940 : INFO : EPOCH 2 - PROGRESS: at 90.96% examples, 656822 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:57,948 : INFO : EPOCH 2 - PROGRESS: at 94.91% examples, 657525 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:20:58,949 : INFO : EPOCH 2 - PROGRESS: at 98.79% examples, 658074 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:20:59,201 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-04-27 13:20:59,206 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-04-27 13:20:59,235 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-04-27 13:20:59,238 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-04-27 13:20:59,245 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-27 13:20:59,252 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 13:20:59,257 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 13:20:59,264 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 13:20:59,265 : INFO : EPOCH - 2 : training on 23584060 raw words (17473992 effective words) took 26.5s, 658190 effective words/s\n",
      "2019-04-27 13:21:00,313 : INFO : EPOCH 3 - PROGRESS: at 3.83% examples, 651284 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:01,322 : INFO : EPOCH 3 - PROGRESS: at 7.64% examples, 651465 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:02,329 : INFO : EPOCH 3 - PROGRESS: at 11.60% examples, 662263 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:03,336 : INFO : EPOCH 3 - PROGRESS: at 15.36% examples, 658130 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:04,353 : INFO : EPOCH 3 - PROGRESS: at 19.47% examples, 667649 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:05,354 : INFO : EPOCH 3 - PROGRESS: at 23.66% examples, 678171 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:06,358 : INFO : EPOCH 3 - PROGRESS: at 27.61% examples, 679165 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:07,373 : INFO : EPOCH 3 - PROGRESS: at 31.39% examples, 676253 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:08,376 : INFO : EPOCH 3 - PROGRESS: at 35.04% examples, 672501 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:09,417 : INFO : EPOCH 3 - PROGRESS: at 38.79% examples, 668444 words/s, in_qsize 16, out_qsize 2\n",
      "2019-04-27 13:21:10,434 : INFO : EPOCH 3 - PROGRESS: at 42.23% examples, 661864 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:11,444 : INFO : EPOCH 3 - PROGRESS: at 45.97% examples, 661700 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:12,447 : INFO : EPOCH 3 - PROGRESS: at 50.07% examples, 665824 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:13,450 : INFO : EPOCH 3 - PROGRESS: at 53.48% examples, 661034 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:14,470 : INFO : EPOCH 3 - PROGRESS: at 57.37% examples, 661483 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:15,480 : INFO : EPOCH 3 - PROGRESS: at 61.25% examples, 662214 words/s, in_qsize 16, out_qsize 1\n",
      "2019-04-27 13:21:16,489 : INFO : EPOCH 3 - PROGRESS: at 64.53% examples, 656944 words/s, in_qsize 13, out_qsize 2\n",
      "2019-04-27 13:21:17,491 : INFO : EPOCH 3 - PROGRESS: at 68.19% examples, 656133 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:18,492 : INFO : EPOCH 3 - PROGRESS: at 72.04% examples, 656577 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:19,510 : INFO : EPOCH 3 - PROGRESS: at 75.39% examples, 653128 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:20,519 : INFO : EPOCH 3 - PROGRESS: at 78.89% examples, 650630 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:21,520 : INFO : EPOCH 3 - PROGRESS: at 82.63% examples, 650579 words/s, in_qsize 16, out_qsize 0\n",
      "2019-04-27 13:21:22,530 : INFO : EPOCH 3 - PROGRESS: at 86.50% examples, 650936 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:23,540 : INFO : EPOCH 3 - PROGRESS: at 89.93% examples, 648540 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:24,540 : INFO : EPOCH 3 - PROGRESS: at 93.27% examples, 645641 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:25,555 : INFO : EPOCH 3 - PROGRESS: at 96.83% examples, 644034 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:26,342 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-04-27 13:21:26,366 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-04-27 13:21:26,369 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-04-27 13:21:26,371 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-04-27 13:21:26,382 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-27 13:21:26,394 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 13:21:26,401 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 13:21:26,411 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 13:21:26,412 : INFO : EPOCH - 3 : training on 23584060 raw words (17474709 effective words) took 27.1s, 644015 effective words/s\n",
      "2019-04-27 13:21:27,482 : INFO : EPOCH 4 - PROGRESS: at 3.25% examples, 545707 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:28,488 : INFO : EPOCH 4 - PROGRESS: at 6.86% examples, 584354 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:29,490 : INFO : EPOCH 4 - PROGRESS: at 10.50% examples, 598734 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:30,501 : INFO : EPOCH 4 - PROGRESS: at 13.99% examples, 598905 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:31,531 : INFO : EPOCH 4 - PROGRESS: at 17.31% examples, 591017 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:32,532 : INFO : EPOCH 4 - PROGRESS: at 20.18% examples, 576528 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:33,541 : INFO : EPOCH 4 - PROGRESS: at 23.98% examples, 587249 words/s, in_qsize 16, out_qsize 2\n",
      "2019-04-27 13:21:34,545 : INFO : EPOCH 4 - PROGRESS: at 27.57% examples, 592034 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:35,547 : INFO : EPOCH 4 - PROGRESS: at 30.77% examples, 588554 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:36,576 : INFO : EPOCH 4 - PROGRESS: at 34.14% examples, 587249 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:37,604 : INFO : EPOCH 4 - PROGRESS: at 37.39% examples, 584836 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:38,615 : INFO : EPOCH 4 - PROGRESS: at 41.14% examples, 590364 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:39,616 : INFO : EPOCH 4 - PROGRESS: at 44.66% examples, 593263 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:40,624 : INFO : EPOCH 4 - PROGRESS: at 48.09% examples, 593383 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:41,644 : INFO : EPOCH 4 - PROGRESS: at 51.64% examples, 594446 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:42,647 : INFO : EPOCH 4 - PROGRESS: at 54.99% examples, 594172 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:43,663 : INFO : EPOCH 4 - PROGRESS: at 58.33% examples, 593075 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:44,666 : INFO : EPOCH 4 - PROGRESS: at 61.75% examples, 593305 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:45,667 : INFO : EPOCH 4 - PROGRESS: at 65.34% examples, 595156 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:46,672 : INFO : EPOCH 4 - PROGRESS: at 68.57% examples, 593715 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:47,727 : INFO : EPOCH 4 - PROGRESS: at 71.90% examples, 591414 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:48,732 : INFO : EPOCH 4 - PROGRESS: at 75.48% examples, 593294 words/s, in_qsize 13, out_qsize 2\n",
      "2019-04-27 13:21:49,764 : INFO : EPOCH 4 - PROGRESS: at 78.41% examples, 588887 words/s, in_qsize 16, out_qsize 0\n",
      "2019-04-27 13:21:50,798 : INFO : EPOCH 4 - PROGRESS: at 81.89% examples, 588798 words/s, in_qsize 16, out_qsize 0\n",
      "2019-04-27 13:21:51,798 : INFO : EPOCH 4 - PROGRESS: at 85.32% examples, 588564 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:21:52,805 : INFO : EPOCH 4 - PROGRESS: at 89.04% examples, 590772 words/s, in_qsize 13, out_qsize 2\n",
      "2019-04-27 13:21:53,806 : INFO : EPOCH 4 - PROGRESS: at 92.02% examples, 588049 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:54,814 : INFO : EPOCH 4 - PROGRESS: at 95.27% examples, 586697 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:55,839 : INFO : EPOCH 4 - PROGRESS: at 98.45% examples, 585101 words/s, in_qsize 16, out_qsize 0\n",
      "2019-04-27 13:21:56,281 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-04-27 13:21:56,285 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-04-27 13:21:56,286 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-04-27 13:21:56,295 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-04-27 13:21:56,302 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-27 13:21:56,305 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 13:21:56,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 13:21:56,309 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 13:21:56,313 : INFO : EPOCH - 4 : training on 23584060 raw words (17473576 effective words) took 29.9s, 584858 effective words/s\n",
      "2019-04-27 13:21:57,378 : INFO : EPOCH 5 - PROGRESS: at 3.21% examples, 540127 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:58,385 : INFO : EPOCH 5 - PROGRESS: at 7.25% examples, 617975 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:21:59,388 : INFO : EPOCH 5 - PROGRESS: at 10.96% examples, 625905 words/s, in_qsize 13, out_qsize 2\n",
      "2019-04-27 13:22:00,395 : INFO : EPOCH 5 - PROGRESS: at 14.84% examples, 636314 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:22:01,416 : INFO : EPOCH 5 - PROGRESS: at 18.40% examples, 630853 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:22:02,422 : INFO : EPOCH 5 - PROGRESS: at 21.82% examples, 624893 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:03,424 : INFO : EPOCH 5 - PROGRESS: at 25.13% examples, 616918 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:04,433 : INFO : EPOCH 5 - PROGRESS: at 29.07% examples, 625963 words/s, in_qsize 16, out_qsize 1\n",
      "2019-04-27 13:22:05,433 : INFO : EPOCH 5 - PROGRESS: at 32.43% examples, 622211 words/s, in_qsize 16, out_qsize 1\n",
      "2019-04-27 13:22:06,470 : INFO : EPOCH 5 - PROGRESS: at 36.18% examples, 623462 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:07,487 : INFO : EPOCH 5 - PROGRESS: at 39.09% examples, 612339 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:22:08,489 : INFO : EPOCH 5 - PROGRESS: at 42.52% examples, 611788 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:09,497 : INFO : EPOCH 5 - PROGRESS: at 46.36% examples, 616663 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:10,509 : INFO : EPOCH 5 - PROGRESS: at 49.81% examples, 615481 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:11,519 : INFO : EPOCH 5 - PROGRESS: at 53.31% examples, 615000 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:12,526 : INFO : EPOCH 5 - PROGRESS: at 56.72% examples, 613763 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:13,587 : INFO : EPOCH 5 - PROGRESS: at 60.33% examples, 612446 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:14,603 : INFO : EPOCH 5 - PROGRESS: at 63.52% examples, 609149 words/s, in_qsize 13, out_qsize 2\n",
      "2019-04-27 13:22:15,605 : INFO : EPOCH 5 - PROGRESS: at 66.89% examples, 608209 words/s, in_qsize 16, out_qsize 0\n",
      "2019-04-27 13:22:16,628 : INFO : EPOCH 5 - PROGRESS: at 69.88% examples, 603404 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:17,641 : INFO : EPOCH 5 - PROGRESS: at 73.02% examples, 600407 words/s, in_qsize 16, out_qsize 0\n",
      "2019-04-27 13:22:18,657 : INFO : EPOCH 5 - PROGRESS: at 76.23% examples, 598577 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:19,664 : INFO : EPOCH 5 - PROGRESS: at 79.65% examples, 598081 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:20,683 : INFO : EPOCH 5 - PROGRESS: at 83.11% examples, 597646 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:22:21,692 : INFO : EPOCH 5 - PROGRESS: at 86.16% examples, 594536 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:22:22,718 : INFO : EPOCH 5 - PROGRESS: at 89.72% examples, 594969 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:23,760 : INFO : EPOCH 5 - PROGRESS: at 93.06% examples, 593350 words/s, in_qsize 15, out_qsize 0\n",
      "2019-04-27 13:22:24,761 : INFO : EPOCH 5 - PROGRESS: at 96.53% examples, 593496 words/s, in_qsize 14, out_qsize 1\n",
      "2019-04-27 13:22:25,657 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-04-27 13:22:25,665 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-04-27 13:22:25,675 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-04-27 13:22:25,682 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-04-27 13:22:25,698 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-27 13:22:25,703 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 13:22:25,705 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 13:22:25,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 13:22:25,707 : INFO : EPOCH - 5 : training on 23584060 raw words (17473130 effective words) took 29.4s, 594919 effective words/s\n",
      "2019-04-27 13:22:25,708 : INFO : training on a 117920300 raw words (87369144 effective words) took 140.4s, 622140 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 57s, sys: 6 s, total: 10min 3s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "\n",
    "# список параметров, которые можно менять по вашему желанию\n",
    "params = {}\n",
    "params['size'] = 300  # итоговая размерность вектора каждого слова\n",
    "params['min_count'] = 5  # минимальная частотность слова, чтобы оно попало в модель\n",
    "params['workers'] = 8     # количество ядер вашего процессора, чтоб запустить обучение в несколько потоков\n",
    "params['window'] = 10        # размер окна \n",
    "params['sample'] = 1e-3 # внутренняя метрика модели\n",
    "\n",
    "model = Word2Vec(sentences, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 13:26:50,507 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "# Финализируем нашу модель. Ее нельзя будет доучить теперь, но она начнет занимать гораздо меньше места\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренировав модель, получили представление каждого слова в семантическом пространстве (часто называют \"псевдо\" семантическое пространство)\n",
    "\n",
    "Попробуем популярный пример: QUEEN + MAN - KING = ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hawking', 0.39587709307670593),\n",
       " ('men', 0.36946797370910645),\n",
       " ('himself', 0.36450180411338806),\n",
       " ('guy', 0.3641517460346222),\n",
       " ('filmmaker', 0.3599601984024048),\n",
       " ('nemesis', 0.3579413890838623),\n",
       " ('soldier', 0.3561764657497406),\n",
       " ('alvin', 0.3560636639595032),\n",
       " ('lad', 0.3560309410095215),\n",
       " ('donner', 0.35371023416519165)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['queen', 'man'], negative=['king'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось WOMAN, что логично :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.854441225528717),\n",
       " ('flick', 0.6861598491668701),\n",
       " ('movies', 0.5816400647163391),\n",
       " ('it', 0.5664041638374329),\n",
       " ('sequel', 0.5207743644714355),\n",
       " ('picture', 0.513239860534668),\n",
       " ('miniseries', 0.4933973550796509),\n",
       " ('documentary', 0.49095818400382996),\n",
       " ('show', 0.483186811208725),\n",
       " ('stinker', 0.478931725025177)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим слова, которые очень похожи на слово MOVIE\n",
    "\n",
    "model.most_similar('movie')\n",
    "\n",
    "# Попробуем и другие слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7712875008583069),\n",
       " ('atrocious', 0.7404935359954834),\n",
       " ('horrible', 0.7230589389801025),\n",
       " ('dreadful', 0.6988893747329712),\n",
       " ('abysmal', 0.6940825581550598),\n",
       " ('appalling', 0.6884695291519165),\n",
       " ('horrid', 0.6619865894317627),\n",
       " ('horrendous', 0.650810956954956),\n",
       " ('lousy', 0.6421493291854858),\n",
       " ('amateurish', 0.6044957637786865)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('awful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('huge', 0.6679580807685852),\n",
       " ('major', 0.4751586318016052),\n",
       " ('biggest', 0.4730166792869568),\n",
       " ('mega', 0.4682440757751465),\n",
       " ('massive', 0.43350180983543396),\n",
       " ('silver', 0.4298832416534424),\n",
       " ('bigger', 0.4158284664154053),\n",
       " ('miniscule', 0.4135412573814392),\n",
       " ('minuscule', 0.4091233015060425),\n",
       " ('savers', 0.3983842134475708)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('email', 0.6582958698272705),\n",
       " ('yahoo', 0.554697573184967),\n",
       " ('mails', 0.5379413366317749),\n",
       " ('html', 0.5329873561859131),\n",
       " ('hotmail', 0.5301001667976379),\n",
       " ('aol', 0.5283646583557129),\n",
       " ('download', 0.5272182822227478),\n",
       " ('matin', 0.5097979307174683),\n",
       " ('magazines', 0.501447856426239),\n",
       " ('letters', 0.49451926350593567)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('mail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moscow'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# так же взглянем на функцию doesnt_match, она покажет лишнее слово в массив\n",
    "\n",
    "model.doesnt_match(['berlin', 'moscow', 'africa', 'rome', 'paris'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(['man', 'woman', 'child', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[-2.41765045e-02  1.13930516e-02 -7.65231401e-02  1.34541765e-01\n",
      " -5.49982674e-02 -1.80851985e-02 -3.10797468e-02 -1.10409237e-01\n",
      "  6.21234216e-02 -9.40666422e-02 -1.31945843e-02  3.60658616e-02\n",
      " -5.46340831e-02 -3.95423314e-03  3.73740755e-02  5.67800663e-02\n",
      " -1.43558607e-01  5.73062785e-02 -1.83172941e-01 -9.55514051e-03\n",
      " -2.78906915e-02  6.34366348e-02  6.81844279e-02 -8.58814269e-02\n",
      "  9.13262460e-03  3.08009293e-02  2.22716555e-02 -6.25756159e-02\n",
      "  1.69782974e-02 -1.98944025e-02  2.02695411e-02  1.42104356e-02\n",
      " -1.42527595e-02 -4.64156531e-02  8.75117034e-02  2.32919585e-02\n",
      " -2.09304038e-02 -3.89328972e-02 -2.72467895e-03 -8.74985158e-02\n",
      "  6.51913062e-02  7.38173425e-02  3.87203507e-02  4.50330600e-03\n",
      "  2.38494184e-02 -6.40513971e-02  3.16905566e-02 -5.52261584e-02\n",
      "  9.94774103e-02 -3.16848904e-02 -8.82579312e-02 -5.37963361e-02\n",
      " -1.58704780e-02 -1.01641469e-01  5.25919488e-03  8.60715210e-02\n",
      "  8.82308558e-02  1.27134249e-01  5.67219593e-02 -3.35409045e-02\n",
      "  5.38940467e-02 -1.20894521e-01  1.41378958e-02 -2.09004269e-03\n",
      " -8.18200633e-02 -3.22752520e-02  1.24665760e-01 -5.60167283e-02\n",
      "  3.82093489e-02 -2.98985317e-02  5.54875918e-02 -4.33332101e-02\n",
      "  2.99909059e-03 -4.85648811e-02 -8.91429111e-02 -7.85294250e-02\n",
      "  1.58066869e-01 -3.81734734e-03 -5.41753657e-02 -2.61609592e-02\n",
      "  1.35440822e-03 -1.45380069e-02  8.68010223e-02 -1.23970723e-02\n",
      " -2.40399484e-02 -6.43646494e-02 -1.40028195e-02  1.08746976e-01\n",
      " -2.03415938e-02 -1.16297114e-03 -3.61369364e-02  4.45549488e-02\n",
      " -1.55177470e-02 -1.17938899e-01  6.07671682e-03 -8.71359706e-02\n",
      "  2.78886519e-02  3.45698893e-02 -1.56604294e-02  9.15265605e-02\n",
      " -3.96372750e-02 -7.32956231e-02  2.21520197e-03  3.76145616e-02\n",
      " -3.37876268e-02 -4.66680750e-02 -5.74119650e-02 -7.54907057e-02\n",
      "  2.73948945e-02 -4.80581522e-02 -8.30582008e-02 -4.33516651e-02\n",
      " -5.61879650e-02 -1.80224627e-02  3.49782072e-02  9.10712034e-03\n",
      "  4.66884784e-02 -3.59747522e-02  6.10550940e-02  4.74155955e-02\n",
      "  2.79946923e-02 -5.55277988e-02 -5.19170985e-03  4.79546152e-02\n",
      " -1.50742866e-02  3.32363807e-02 -4.24826816e-02 -3.52726616e-02\n",
      " -4.36679646e-02 -2.47732289e-02  2.08934247e-02 -4.66363057e-02\n",
      " -6.07669279e-02  6.30302355e-03 -9.28642303e-02 -2.59194132e-02\n",
      "  4.00596447e-02 -7.62143433e-02 -1.52037829e-01  1.59968790e-02\n",
      " -1.06443003e-01 -7.15043321e-02 -8.62560980e-03 -3.59041952e-02\n",
      " -1.05989436e-02  7.70455506e-03  2.95024551e-02  6.27577305e-02\n",
      " -1.59491822e-02 -7.95388296e-02  5.99360168e-02 -1.47044882e-02\n",
      "  1.47744799e-02  5.80875687e-02  2.99807806e-02 -7.52171353e-02\n",
      " -7.21249636e-03 -2.25498527e-02  5.80690131e-02 -2.11796351e-02\n",
      " -2.40333006e-02  4.29081768e-02 -8.55446085e-02 -3.95520665e-02\n",
      " -5.37490435e-02  2.85650045e-02 -2.85069197e-02 -7.10773095e-02\n",
      "  2.90800333e-02  1.84432901e-02  1.22240156e-01  2.44552325e-02\n",
      " -3.65718193e-02 -5.97423175e-04 -4.16676067e-02  8.75877738e-02\n",
      " -8.66911784e-02 -3.67379524e-02  2.75897421e-02 -5.22890314e-02\n",
      "  4.46765311e-02  7.57307634e-02  1.65343791e-01  5.73498681e-02\n",
      "  1.61457004e-03 -1.57019440e-02 -2.29621548e-02  7.55722448e-02\n",
      "  2.86095291e-02 -3.22782509e-02 -5.80734275e-02  8.60865787e-02\n",
      " -2.86258925e-02 -4.52639312e-02 -9.77531597e-02  5.31401560e-02\n",
      " -1.24832124e-01 -1.69444811e-02  7.60833472e-02 -6.83333203e-02\n",
      "  4.36861515e-02 -7.66946226e-02  1.65500399e-02  2.28989534e-02\n",
      " -6.25582878e-03  3.80596556e-02 -8.95959511e-03 -1.25725502e-02\n",
      " -2.22928431e-02 -1.81384087e-02  3.88103724e-02  5.42474026e-03\n",
      " -5.87319722e-03 -7.25725712e-03  8.95641893e-02 -1.85120981e-02\n",
      " -3.20280306e-02  1.95996314e-02 -3.65913697e-02  9.12221670e-02\n",
      "  4.29393351e-02  8.47692341e-02 -4.42852117e-02 -7.73902833e-02\n",
      "  6.08679652e-02  2.90804729e-02 -7.11187869e-02 -1.39413163e-01\n",
      "  2.64126565e-02 -5.39904386e-02  9.91241038e-02 -1.00118771e-01\n",
      " -5.11555709e-02 -4.08919752e-02  1.02402657e-01  7.55556300e-03\n",
      " -1.08036049e-01 -5.05017377e-02  4.44730595e-02  4.92236093e-02\n",
      "  2.85100751e-02 -4.00556922e-02 -4.80068251e-02  4.60750237e-02\n",
      "  2.44907923e-02  1.39008369e-02 -3.93698318e-03  3.79597060e-02\n",
      " -9.44181997e-03 -2.52135415e-02  5.61037920e-02  2.12190527e-04\n",
      " -2.20083934e-03 -2.81505436e-02  6.24583252e-02 -3.06124333e-03\n",
      "  2.24010907e-02  2.81446502e-02 -8.58347118e-02  4.36804742e-02\n",
      " -5.82337081e-02  3.96117009e-02 -1.24361023e-01 -5.96120628e-03\n",
      "  5.61600178e-02  2.06088293e-02  4.71603386e-02  9.41201150e-02\n",
      "  1.65284742e-02 -1.15783140e-02  1.79586299e-02  1.09876014e-01\n",
      "  5.99705651e-02 -3.53492498e-02 -1.12238014e-02 -3.08901295e-02\n",
      " -3.23718004e-02  6.52407333e-02 -5.27592115e-02 -5.68056293e-02\n",
      " -4.04644385e-02 -1.29509225e-01 -4.24720980e-02  7.89282261e-04\n",
      "  9.15790815e-03  1.78312846e-02  1.67333297e-02 -5.76807465e-03\n",
      "  2.94657294e-02 -7.36071244e-02  2.41809450e-02 -2.06615627e-02\n",
      " -5.28501123e-02 -1.37982352e-04  5.80998510e-02  4.89117689e-02\n",
      "  1.96451060e-02 -9.65288505e-02 -4.25457805e-02 -4.72014770e-03]\n"
     ]
    }
   ],
   "source": [
    "# давайте просмотрим вектор одного из слов\n",
    "# его длину\n",
    "print (len(model['moon']))\n",
    "\n",
    "# и сам вектор\n",
    "print (model['moon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53172"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Словарь - все слова которые участвуют в модели можно просмотреть так\n",
    "len(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. мы хотим классифицировать не слова, а тексты, надо перевести тексты в вектора (представить в виде фич)\n",
    "Один из простых методов - сложить все вектора слов входящих в текст и поделить на число входящих слов.\n",
    "Напишем функцию, которая:\n",
    "* создает нулевой вектор - это будет результирующий вектор\n",
    "* идем по всем словам в тексте, если слово есть в моделе:\n",
    "  * увеличиваем счетчик слов\n",
    "  * прибавим вектор слова к результирующему вектору\n",
    "* поделим все координаты на число слов, вектора которых мы прибавляли к результирующему вектору"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vec(words, model, size):\n",
    "    text_vec = np.zeros((size,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "\n",
    "    index2word_set = set(model.wv.vocab.keys())\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words = n_words + 1\n",
    "            text_vec = np.add(text_vec, model[word])\n",
    "    \n",
    "    if n_words != 0:\n",
    "        text_vec /= n_words\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая на входе получает список всех текстов, а на выходе отдает список вектор каждого текста - что является прямоугольной матрицей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def texts_to_vecs(texts, model, size):\n",
    "    texts_vecs = np.zeros((len(texts), size), dtype=\"float32\")\n",
    "    \n",
    "    for i, text in tqdm(enumerate(texts),):\n",
    "        texts_vecs[i] = text_to_vec(text, model, size)\n",
    "\n",
    "    return texts_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим что функция texts_to_vecs принимает не просто тексты, а список всех слов текста. \n",
    "\n",
    "(!!!) Внимание: не список списков (там где сначала делили на предложения, а предложения на слова), а обычный линейный список\n",
    "\n",
    "Но у нас есть функции, которые переводят 1) текст в список предложений, 2) предложение в список слов\n",
    "\n",
    "Может показаться, что можно использовать 2ую функцию, но придется тогда ее переписать, потому как теги у нас удаляются лишь в первой функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Поступим иначе, в python есть возможность развернуть двухмерный массив в одномерный, вот пример\n",
    "\n",
    "temp_list = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]\n",
    "print (sum(temp_list, []))\n",
    "\n",
    "# магия :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t длина list_of_list_of_words\n",
      "3\n",
      "\t первый элемент list_of_list_of_words\n",
      "['sherlock', 'holmes', 'is', 'a', 'four', 'act', 'play', 'written', 'by', 'william', 'gillette', 'and', 'sir', 'arthur', 'conan', 'doyle', 'based', 'on', 'conan', 'doyle', 's', 'eponymous', 'character']\n",
      "\t длина list_of_words\n",
      "74\n",
      "\t первый элемент list_of_words\n",
      "sherlock\n"
     ]
    }
   ],
   "source": [
    "# Мы же такой возможностью воспользуемся, зная что функция text_to_sentences возвращает список списков\n",
    "\n",
    "list_of_list_of_words = text_to_sentences(\"\"\"\n",
    "Sherlock Holmes is a four-act play written by <p/> William Gillette and Sir Arthur Conan Doyle, \n",
    "based on Conan Doyle's eponymous character. It drew material from the stories \n",
    "<s>\"A Scandal in Bohemia\"</s>, \"The Final Problem\", and A Study in Scarlet, pitting Holmes \n",
    "against Professor Moriarty and reinventing the character of Irene Adler as a new love \n",
    "interest named Alice Faulkner. This play introduced the phrase \"Elementary, my dear Watson\" \n",
    "and Holmes' curved pipe.\n",
    "\"\"\")\n",
    "\n",
    "print ('\\t длина list_of_list_of_words')\n",
    "print (len(list_of_list_of_words))\n",
    "print ('\\t первый элемент list_of_list_of_words')\n",
    "print (list_of_list_of_words[0])\n",
    "\n",
    "list_of_words = sum(list_of_list_of_words, [])\n",
    "\n",
    "print ('\\t длина list_of_words')\n",
    "print (len(list_of_words))\n",
    "print ('\\t первый элемент list_of_words')\n",
    "print (list_of_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45412419fed844da935fe7624e448077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# действительно работает, сделаем для всех текстов из train\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "train_like_word_list = [sum(text_to_sentences(text), []) for text in tqdm(train['review'], total=len(train['review']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843a7bf76a264c9fbd02f3e8cf5a53bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_vecs = texts_to_vecs(train_like_word_list, model, params['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d6285c17054e7abaa90b928c3091be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# сделаем тоже самое для test\n",
    "test_like_word_list = [sum(text_to_sentences(text), []) for text in tqdm(test['review'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfd5a3549154703a0eb521450afb301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_vecs = texts_to_vecs(test_like_word_list, model, params['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9130505555594812\n",
      "CPU times: user 31 s, sys: 132 ms, total: 31.1 s\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Воспользуемся train_vecs, test_vecs, train[\"sentiment\"] \n",
    "#    как матрица фичей обучающей выборки, матрица фичей тестовой выборки, таргет для обучающей выборки соответственно\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_vecs,  train[\"sentiment\"])\n",
    "# Стандартный случайный лес в таком случае\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(X_train,y_train)\n",
    "predict = forest.predict_proba(X_test)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_test, predict[:,1]))\n",
    "# И вот задача решена"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но что если для получения результирующего вектора не складывать вектора, а пойти другим способом.\n",
    "\n",
    "Кластеризуем все слова на 1000 классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В model.syn0 хранятся все вектора. Кластеризуем их!\n",
    "\n",
    "print ('Размер ', model.wv.syn0.shape)\n",
    "print ('Вектор ', model.wv.syn0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \"\"\"\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    897\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initialization complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m     centers, labels, n_iter = k_means_elkan(X, n_clusters, centers, tol=tol,\n\u001b[0;32m--> 400\u001b[0;31m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minertia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/cluster/_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/envs/bd9/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m     \"\"\"\n\u001b[1;32m   1780\u001b[0m     \u001b[0mSum\u001b[0m \u001b[0mof\u001b[0m \u001b[0marray\u001b[0m \u001b[0melements\u001b[0m \u001b[0mover\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Кластеризируем все слова. \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "word_vectors = model.wv.syn0\n",
    "# Число кластеров установим в 1000. Для этого числа нет \"серебряной пули\". Для каждого случая лучше подойдет разная\n",
    "num_clusters = 1000\n",
    "\n",
    "# Начнем кластеризацию, учитывая что классов много, количество векторов (по сути слов) много,\n",
    "#   все это будет происходит продолжительное время. Можно сходить за чаем.\n",
    "kmeans_clustering = KMeans(n_clusters=num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "# в idx будут храниться номера классов для каждого слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим структуру dict (словарь): слово -> класс\n",
    "word_centroid_map = dict(zip(model.wv.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(word_centroid_map,open('word_centroid_map.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "word_centroid_map = pickle.load(open('word_centroid_map.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_centroid_map['rhyming']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# выведем представителей первых 10 классов и посмотрим на адекватность произошедшей кластеризации\n",
    "clust = list(word_centroid_map.values())\n",
    "wrds = list(word_centroid_map.keys())\n",
    "for cluster in range(0,10):\n",
    "    print (cluster)\n",
    "    words = []\n",
    "\n",
    "    for i in range(0, len(clust)):\n",
    "        if clust[i] == cluster:\n",
    "            words.append(wrds[i])\n",
    "    print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует несколько подходов к работе с кластерами слов. Рассмотрим 2 примера\n",
    "* 1) посчитаем для каждого текста вектор, сколько его слов встретилось в каждом кластере. Т.е. для каждого текста будет вектор размера, равного числу кластеров, значениями вектора будут натуральные числа.\n",
    "* 2) Посчитаем усредненных удаленность векторов текстов от каждого центроида всех кластеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для первого случая напишем функцию, на вход которой поступает текст, представленный в виде списка всех его слов\n",
    "#    смотрим в каком кластере находится каждое слово \n",
    "#    и увеличиваем соответствующую ячейку (ответственную за этот кластер) на 1\n",
    "\n",
    "\n",
    "def create_bag_of_centroids(wordlist, word_centroid_map, num_centroids):\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    set_word_centroid_map = set(word_centroid_map.keys())\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in set_word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    \n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Но нам нужно это не для одного текста, а для всех текстов обучающей и тестовой выборки\n",
    "# Сделаем это\n",
    "\n",
    "train_vecs_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, text in enumerate(train_like_word_list):\n",
    "    train_vecs_centroids[i] = create_bag_of_centroids(text, word_centroid_map, num_clusters)\n",
    "\n",
    "test_vecs_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, text in enumerate(test_like_word_list):\n",
    "    test_vecs_centroids[i] = create_bag_of_centroids(text, word_centroid_map, num_clusters)\n",
    "    \n",
    "# Результатом будет матрицы для обучающих и тестовых текстов, что будет матрицей фич обучающей и тестовой выборок соотв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартный случайный лес на полученных матрицах\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs_centroids, train[\"sentiment\"])\n",
    "predict = forest.predict(test_vecs_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# а теперь попробуем посчитать растояние от центроидов\n",
    "\n",
    "train_vecs_centroids_dist = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "for i, vec in tqdm(enumerate(train_vecs), total=len(train_vecs)):\n",
    "    for j, center in enumerate(kmeans_clustering.cluster_centers_):\n",
    "        train_vecs_centroids_dist[i][j] = distance.euclidean(vec, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs_centroids_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs_centroids_dist = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "for i, vec in tqdm(enumerate(test_vecs), total=len(test_vecs)):\n",
    "    for j, center in enumerate(kmeans_clustering.cluster_centers_):\n",
    "        test_vecs_centroids_dist[i][j] = distance.euclidean(vec, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vecs_centroids_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартный случайный лес на полученных матрицах (кто бы сомневался)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, n_jobs=8)\n",
    "forest = forest.fit(train_vecs_centroids_dist, train[\"sentiment\"])\n",
    "predict = forest.predict(test_vecs_centroids_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
