{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--conf spark.sql.catalogImplementation=in-memory pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![big_picture](https://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![programming_model](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![quick_example](https://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/pavel.klemenkov/lectures/lecture05/dummy_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -tail /user/pavel.klemenkov/lectures/lecture05/dummy_files/file01.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -tail /user/pavel.klemenkov/lectures/lecture05/dummy_files/file02.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -tail /user/pavel.klemenkov/lectures/lecture05/dummy_files/file03.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(fields=[\n",
    "    StructField(\"animal\", StringType()),\n",
    "    StructField(\"count\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/user/pavel.klemenkov/lectures/lecture05/dummy_files/file01.csv\", sep=\"\\t\", schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a streaming DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream\\\n",
    "                    .schema(schema)\\\n",
    "                    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "                    .csv(\"/user/pavel.klemenkov/lectures/lecture05/dummy_files/\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some transformations on this DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation = streaming_df.select(\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the output stream. `start()` method starts streaming computation and returns a `StreamingQuery`. It does not block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = computation.writeStream\\\n",
    "                   .format(\"memory\")\\\n",
    "                   .queryName(\"some_query\")\\\n",
    "                   .outputMode(\"append\")\\\n",
    "                   .trigger(processingTime=\"30 seconds\")\\\n",
    "                   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    spark.sql(\"select * from some_query\").show()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check whether query is still running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the information about batches is available via `recentProgess` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.recentProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output modes\n",
    "\n",
    "The “Output” is defined as what gets written out to the external storage. The output can be defined in a different mode:\n",
    "\n",
    "**Complete Mode** - The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.\n",
    "\n",
    "**Append Mode** - Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\n",
    "\n",
    "**Update Mode** - Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream\\\n",
    "                    .schema(schema)\\\n",
    "                    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "                    .csv(\"/user/pavel.klemenkov/lectures/lecture05/dummy_files/\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computation = streaming_df.groupBy(\"animal\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = computation.writeStream\\\n",
    "                   .format(\"memory\")\\\n",
    "                   .queryName(\"some_query\")\\\n",
    "                   .outputMode(\"complete\")\\\n",
    "                   .trigger(processingTime=\"30 seconds\")\\\n",
    "                   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    spark.sql(\"select * from some_query\").show()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n",
    "query.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 15, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/pavel.klemenkov/lectures/lecture05/events|head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 50 JSON files in the directory. Let's see what each JSON file contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -tail /user/pavel.klemenkov/lectures/lecture05/events/file-0.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in the file contains JSON record with two fields - `time` and `action`. Let's try to analyze these files interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch/Interactive Processing\n",
    "The usual first step in attempting to process the data is to interactively query the data. Let's define a static DataFrame on the files, and give it a table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "inputPath = \"/user/pavel.klemenkov/lectures/lecture05/events/\"\n",
    "\n",
    "# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\n",
    "jsonSchema = StructType(fields = [ \n",
    "    StructField(\"time\", TimestampType(), True), \n",
    "    StructField(\"action\", StringType(), True) \n",
    "])\n",
    "\n",
    "# Static DataFrame representing data in the JSON files\n",
    "staticInputDF = spark.read\\\n",
    "                     .schema(jsonSchema)\\\n",
    "                     .json(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticInputDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the number of \"open\" and \"close\" actions with one hour windows. To do this, we will group by the `action` column and 1 hour windows over the `time` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticCountsDF = staticInputDF\\\n",
    "                  .groupBy(staticInputDF.action, f.window(staticInputDF.time, \"1 hour\"))\\\n",
    "                  .count()\n",
    "        \n",
    "staticCountsDF.cache()\n",
    "\n",
    "# Register the DataFrame as table 'static_counts'\n",
    "staticCountsDF.createOrReplaceTempView(\"static_counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can directly use SQL to query the table. For example, here are the total counts across all the hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staticCountsDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select action, sum(count) as total_count from static_counts group by action\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = spark.sql(\"select action, sum(count) as total_count from static_counts group by action\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"action\", y=\"total_count\", data=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a timeline of windowed counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count \n",
    "           from static_counts order by time, action\n",
    "        \"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sns.barplot(x=\"time\", y=\"count\", hue=\"action\", data=pdf)\n",
    "for item in f.get_xticklabels():\n",
    "    item.set_rotation(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the two ends of the graph. The close actions are generated such that they are after the corresponding open actions, so there are more \"opens\" in the beginning and more \"closes\" in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing \n",
    "Now that we have analyzed the data interactively, let's convert this to a streaming query that continuously updates as data comes. Since we just have a static set of files, we are going to emulate a stream from them by reading one file at a time, in the chronological order they were created. The query we have to write is pretty much the same as the interactive query above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "streamingInputDF = spark.readStream\\\n",
    "                        .schema(jsonSchema)\\\n",
    "                        .option(\"maxFilesPerTrigger\", 1)\\\n",
    "                        .json(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same query as staticInputDF\n",
    "streamingCountsDF = streamingInputDF\\\n",
    "                     .groupBy(streamingInputDF.action, f.window(streamingInputDF.time, \"1 hour\"))\\\n",
    "                     .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this DF actually a streaming DF?\n",
    "streamingCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `streamingCountsDF` is a streaming Dataframe (`streamingCountsDF.isStreaming` was `true`). You can start streaming computation, by defining the sink and starting it. \n",
    "In our case, we want to interactively query the counts (same queries as above), so we will set the complete set of 1 hour counts to be in a in-memory table (note that this for testing purpose only in Spark 2.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query = streamingCountsDF.writeStream\\\n",
    "                                   .format(\"memory\")\\\n",
    "                                   .queryName(\"counts\")\\\n",
    "                                   .outputMode(\"complete\")\\\n",
    "                                   .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`query` is a handle to the streaming query that is running in the background. This query is continuously picking up files and updating the windowed counts. \n",
    "\n",
    "Note the status of query in the above cell. The progress bar shows that the query is active. \n",
    "Furthermore, if you expand the `> counts` above, you will find the number of files they have already processed. \n",
    "\n",
    "Let's wait a bit for a few files to be processed and then interactively query the in-memory `counts` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    clear_output()\n",
    "    spark.sql(query).show()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://spark.apache.org/docs/latest/img/structured-streaming-late-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define watermark delays on both inputs such that the engine knows how delayed the input can be (similar to streaming aggregations)\n",
    "\n",
    "2. Define a constraint on event-time across the two inputs such that the engine can figure out when old rows of one input is not going to be required (i.e. will not satisfy the time constraint) for matches with the other input. This constraint can be defined in one of the two ways.\n",
    "\n",
    " 1. Time range join conditions (e.g. ...JOIN ON leftTime BETWEEN rightTime AND rightTime + INTERVAL 1 HOUR),\n",
    "\n",
    " 2. Join on event-time windows (e.g. ...JOIN ON leftTimeWindow = rightTimeWindow).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream-to-stream joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions = (\n",
    "  spark\n",
    "    .readStream.format(\"rate\").option(\"rowsPerSecond\", \"5\").option(\"numPartitions\", \"1\").load()\n",
    "    .selectExpr(\"value AS adId\", \"timestamp AS impressionTime\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks = (\n",
    "  spark\n",
    "  .readStream.format(\"rate\").option(\"rowsPerSecond\", \"5\").option(\"numPartitions\", \"1\").load()\n",
    "  .where((rand() * 100).cast(\"integer\") < 30)\n",
    "  .selectExpr(\"(value - 50) AS adId \", \"timestamp AS clickTime\")\n",
    "  .where(\"adId > 0\")\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = impressions.writeStream\\\n",
    "               .format(\"memory\")\\\n",
    "               .queryName(\"imps\")\\\n",
    "               .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from imps order by impressionTime desc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps_clicks = impressions.join(clicks, on=\"adId\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps_clicks_q = imps_clicks.writeStream\\\n",
    "                           .format(\"memory\")\\\n",
    "                           .queryName(\"imps_clicks_q\")\\\n",
    "                           .start()\n",
    "imps_clicks_q.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from imps_clicks_q order by impressionTime desc\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps_clicks_q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Define watermarks\n",
    "impressionsWithWatermark = impressions \\\n",
    "  .selectExpr(\"adId AS impressionAdId\", \"impressionTime\") \\\n",
    "  .withWatermark(\"impressionTime\", \"10 seconds \")\n",
    "\n",
    "clicksWithWatermark = clicks \\\n",
    "  .selectExpr(\"adId AS clickAdId\", \"clickTime\") \\\n",
    "  .withWatermark(\"clickTime\", \"20 seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps_clicks_watermark_outer = impressionsWithWatermark.join(\n",
    "                                    clicksWithWatermark,\n",
    "                                    expr(\"\"\" \n",
    "                                          clickAdId = impressionAdId AND \n",
    "                                          clickTime >= impressionTime AND \n",
    "                                          clickTime <= impressionTime + interval 1 minutes    \n",
    "                                         \"\"\"\n",
    "                                        ),\n",
    "                                    \"leftOuter\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps_clicks_watermark_outer_q = imps_clicks_watermark_outer.writeStream\\\n",
    "                                                           .format(\"memory\")\\\n",
    "                                                           .queryName(\"imps_clicks_watermark_outer_q\")\\\n",
    "                                                           .start()\n",
    "imps_clicks_watermark_outer_q.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    clear_output()\n",
    "    spark.sql(\"select * from imps_clicks_watermark_outer_q where clickAdId is null\").show(truncate=False)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps_clicks_watermark_outer_q.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-29-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-30-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-31-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-32-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-33-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-34-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-35-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-38-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-39-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pics/apache-spark-20-a-deep-dive-into-structured-streaming-by-tathagata-das-40-1024.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concurrent streaming computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = spark.readStream\\\n",
    "              .format(\"rate\")\\\n",
    "              .option(\"rowsPerSecond\", \"5\")\\\n",
    "              .option(\"numPartitions\", \"1\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool1\")\n",
    "q1 = stream.select(\"timestamp\")\\\n",
    "           .writeStream\\\n",
    "           .format(\"parquet\")\\\n",
    "           .option(\"checkpointLocation\", \"/user/pavel.klemenkov/checkpoint1\")\\\n",
    "           .outputMode(\"append\")\\\n",
    "           .start(\"/user/pavel.klemenkov/q1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"pool2\")\n",
    "q2 = stream.select(\"timestamp\")\\\n",
    "           .writeStream\\\n",
    "           .format(\"parquet\")\\\n",
    "           .option(\"checkpointLocation\", \"/user/pavel.klemenkov/checkpoint2\")\\\n",
    "           .outputMode(\"append\")\\\n",
    "           .start(\"/user/pavel.klemenkov/q2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The same could be achieved in Python using `concurrent.futures` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupported Operations\n",
    "\n",
    "+ Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming Datasets.\n",
    "\n",
    "+ Limit and take first N rows are not supported on streaming Datasets.\n",
    "\n",
    "+ Distinct operations on streaming Datasets are not supported.\n",
    "\n",
    "+ Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.\n",
    "\n",
    "+ Few types of outer joins on streaming Datasets are not supported. See the support matrix in the Join Operations section for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "structured-streaming-python.html",
  "notebookId": 4427805701920676
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
