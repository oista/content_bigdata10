{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основная гипотеза - рекомендовать похожие товары. Искать похожие товары непосредственно в том же каталоге, где находится и основной товар. Сравнивать (краткое описание + основные характеристики) товаров косинусной мерой, где мера похожести будет соответствовать весу рекомендуемого товара. \n",
    "Файл \"ozon_test.txt\" также должен лежать в каталоге пользователя по умолчанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "#### Да-да, куда без Спарка?\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгружаем кучу либ, почти всё понадобится\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "import pyspark.sql.functions as f\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем основные данные для получения успешного резульата - описание и характеристики товаров магазина\n",
    "data = spark.read.json('/labs/project02/item_details_full')\n",
    "data=data.dropDuplicates()   # Удаляем дубликаты. Как ни странно, но они есть\n",
    "data=data.fillna(' ')        # Заполняем пробелами пустые attr#, чтобы потом нормально склеить\n",
    "data.cache()                 # На всякий случай кэшим (видимо здесь меня сейчас пнут по голове..)\n",
    "data.registerTempTable('item_det')  # SQL рулит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кушаем все реквизиты кроме основного описания. Во-первых, оно большое, во-вторых ни о чём\n",
    "item1 = spark.sql('''SELECT attr1||attr10||attr11||attr12||attr13||attr14||attr15||attr16||attr17||attr18||attr19||attr2||\n",
    "                   attr20||attr21||attr22||attr23||attr24||attr25||attr26||attr27||attr28||attr29||attr3||attr30||attr31|| \n",
    "                   attr32||attr33||attr34||attr35||attr36||attr37||attr38||attr39||attr4||attr40||attr41||attr42||attr43||\n",
    "                   attr44||attr45||attr46||attr47||attr48||attr49||attr5||attr50||attr51||attr52||attr53||attr54||attr55||\n",
    "                   attr56||attr57||attr58||attr59||attr6||attr60||attr61||attr62||attr63||attr64||attr65||attr66||attr67||\n",
    "                   attr68||attr69||attr7||attr70||attr71||attr72||attr73||attr74||attr75||attr76||attr77||attr8||attr9 as attr,\n",
    "                   itemid,parent_id FROM item_det''')\n",
    "item1.registerTempTable('item1')  # SQL рулит снова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем второй основной файл - соответствие \"id товара\"-\"id каталога\"\n",
    "schema_t = StructType(fields=[StructField(\"itemid\",StringType()),StructField(\"catalogid\",StringType())])\n",
    "item_cat = spark.read.json(\"/labs/project02/catalogs\",schema=schema_t)\n",
    "item_cat.registerTempTable('catalogs')  # SQL рулит опять"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скрестим сами товары с каталогами, к которым они относятся\n",
    "item2 = spark.sql('''SELECT trim(a.attr) as attr,a.itemid,b.catalogid FROM item1 a, catalogs b where a.itemid=b.itemid''')\n",
    "item2.registerTempTable('item2')  # SQL рулит обратно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим тестовые товары\n",
    "schema_test = StructType(fields=[StructField(\"item\",StringType())])\n",
    "item_test = spark.read.json(\"/labs/project02/ozon_test.txt\",schema=schema_test)\n",
    "item_test.registerTempTable('test')  # SQL рулит ещё раз"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Токенизация\n",
    "@f.udf(ArrayType(StringType()))\n",
    "def re_tokenizer(text):\n",
    "    regex = re.compile(r'[\\w\\d]{2,}', re.U)\n",
    "    return regex.findall(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим TF-IDF для всех товаров магазина\n",
    "wordsData_udf = item2.withColumn('words', re_tokenizer('attr'))\n",
    "tokenizer = Tokenizer(inputCol=\"attr\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(item2)\n",
    "hashingTF = HashingTF(inputCol=\"words\",\n",
    "                      outputCol=\"TFFeatures\",\n",
    "                      numFeatures=10000, )\n",
    "featurizedData = hashingTF.transform(wordsData_udf)\n",
    "idf = IDF(inputCol=\"TFFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "from pyspark.ml.feature import Normalizer\n",
    "t = Normalizer(inputCol='features', outputCol='norm_features', p=2.0)\n",
    "normalizedData = t.transform(rescaledData)\n",
    "# Уф.. Отмучались (рука бойца устала копипастить лабу 7)\n",
    "normalizedData1=normalizedData.drop('attr','words','TFFeatures','features')  # Удаляем лишние колонки\n",
    "normalizedData2=normalizedData1.cache()  # Вот здесь точно надо!\n",
    "normalizedData2.registerTempTable('nd')  # Ну и ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors, Matrices\n",
    "def cosine_similarity(a,b):\n",
    "    dot = a.dot(b)\n",
    "    return str(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Скрестим товары из теста с каталогами. \n",
    "# Их будет больше, чем просто товаров в тесте, так как многие товары имеют несколько каталогов\n",
    "item_cat2= spark.sql('select * from catalogs where itemid in (SELECT item FROM test)') \n",
    "item_cat2.registerTempTable('item_cat2') # SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Побежим по каталогам, заодно и не сломаемся, если у какого-то товара не будет каталога\n",
    "test=spark.sql('select a.catalogid,count(a.catalogid) as cnt from item_cat2 a group by a.catalogid order by count(catalogid) desc')\n",
    "test=test.toPandas()  # не успел понять, как бежать по SparkDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/3571 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/75 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▏         | 1/75 [00:01<02:10,  1.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|▎         | 2/75 [00:03<02:08,  1.76s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|▍         | 3/75 [00:05<02:04,  1.72s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|▌         | 4/75 [00:06<01:59,  1.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 5/75 [00:08<02:01,  1.73s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|▊         | 6/75 [00:10<02:00,  1.74s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  9%|▉         | 7/75 [00:11<01:55,  1.70s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█         | 8/75 [00:13<01:51,  1.67s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▏        | 9/75 [00:14<01:48,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 10/75 [00:16<01:46,  1.64s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|█▍        | 11/75 [00:17<01:43,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|█▌        | 12/75 [00:19<01:41,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 17%|█▋        | 13/75 [00:21<01:40,  1.63s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█▊        | 14/75 [00:22<01:39,  1.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 15/75 [00:24<01:36,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|██▏       | 16/75 [00:25<01:35,  1.61s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██▎       | 17/75 [00:27<01:32,  1.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 24%|██▍       | 18/75 [00:28<01:31,  1.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 19/75 [00:30<01:29,  1.59s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 20/75 [00:31<01:26,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 28%|██▊       | 21/75 [00:33<01:25,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|██▉       | 22/75 [00:34<01:23,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 31%|███       | 23/75 [00:36<01:22,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|███▏      | 24/75 [00:37<01:20,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███▎      | 25/75 [00:39<01:18,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 35%|███▍      | 26/75 [00:40<01:16,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|███▌      | 27/75 [00:42<01:15,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|███▋      | 28/75 [00:43<01:13,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 39%|███▊      | 29/75 [00:45<01:11,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 30/75 [00:47<01:10,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|████▏     | 31/75 [00:48<01:08,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 43%|████▎     | 32/75 [00:50<01:07,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████▍     | 33/75 [00:51<01:05,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████▌     | 34/75 [00:53<01:04,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 35/75 [00:54<01:02,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|████▊     | 36/75 [00:56<01:00,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|████▉     | 37/75 [00:57<00:59,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|█████     | 38/75 [00:59<00:58,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|█████▏    | 39/75 [01:01<00:56,  1.57s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 40/75 [01:02<00:54,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|█████▍    | 41/75 [01:04<00:53,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████▌    | 42/75 [01:05<00:51,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 57%|█████▋    | 43/75 [01:06<00:49,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████▊    | 44/75 [01:08<00:48,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 45/75 [01:09<00:46,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 61%|██████▏   | 46/75 [01:11<00:45,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|██████▎   | 47/75 [01:13<00:43,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|██████▍   | 48/75 [01:14<00:42,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 65%|██████▌   | 49/75 [01:16<00:40,  1.56s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████▋   | 50/75 [01:17<00:38,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████▊   | 51/75 [01:19<00:37,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 69%|██████▉   | 52/75 [01:20<00:35,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████   | 53/75 [01:22<00:34,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 72%|███████▏  | 54/75 [01:23<00:32,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 55/75 [01:24<00:30,  1.55s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▍  | 56/75 [01:26<00:29,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 76%|███████▌  | 57/75 [01:27<00:27,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|███████▋  | 58/75 [01:29<00:26,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████▊  | 59/75 [01:30<00:24,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 60/75 [01:32<00:23,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████▏ | 61/75 [01:33<00:21,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 83%|████████▎ | 62/75 [01:35<00:19,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████▍ | 63/75 [01:36<00:18,  1.53s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|████████▌ | 64/75 [01:38<00:16,  1.54s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 65/75 [01:42<00:15,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 66/75 [01:44<00:14,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████▉ | 67/75 [01:45<00:12,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 91%|█████████ | 68/75 [01:47<00:11,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████▏| 69/75 [01:49<00:09,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 70/75 [01:50<00:07,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|█████████▍| 71/75 [01:52<00:06,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|█████████▌| 72/75 [01:54<00:04,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|█████████▋| 73/75 [01:55<00:03,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████▊| 74/75 [01:56<00:01,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 75/75 [01:58<00:00,  1.58s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 210/3571 [02:00<32:12,  1.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 3571/3571 [02:00<00:00, 29.57it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "with open('project02.txt','w') as fil:\n",
    "    for i in tqdm(range(test['cnt'].count())):  #бежим по каталогам. Бежать вне каталогов оочень дорого..\n",
    "        if test['cnt'][i]==75:     #берем каталоги с 76 товарами (для теста). В боевых условиях закомментировать!\n",
    "            nd = spark.sql('select * from nd where catalogid=%i'%int(test['catalogid'][i]))  #выбрали товары только по текущему каталогу          \n",
    "            nd.cache()  # Здесь тоже точно нужно!\n",
    "            nd.registerTempTable('courses') # Всё ещё SQL рулит\n",
    "            item_cat3=item_cat2.filter('catalogid=%i'%int(test['catalogid'][i]))   #отберём товары теста текущей группы\n",
    "            item_cat3=item_cat3.toPandas() # Ремарка пунктом выше\n",
    "            for i in tqdm(range(item_cat3['itemid'].count())): #пробежимся по всем товарам теста текущей группы\n",
    "                chosen_doc = spark.sql('SELECT * FROM courses WHERE itemid = %i' % int(item_cat3['itemid'][i])).first()\n",
    "                # для каждого chosen_doc найдём косинусную меру товаров из его (товара из теста) группы\n",
    "                sqlContext.registerFunction('cosine_similarity',\n",
    "                    lambda x: cosine_similarity(x, chosen_doc.norm_features)\n",
    "                )\n",
    "                resultData = spark.sql(\n",
    "                    '''\n",
    "                    SELECT itemid, round(cosine_similarity(norm_features)*100000,2) as similarity \n",
    "                    FROM courses \n",
    "                    ORDER BY similarity DESC\n",
    "                    '''\n",
    "                )\n",
    "                # возьмём столько значений, сколько нам надо и положим сразу в файл, а то всякое бывает, Spark же (не rdd же)\n",
    "                pn={'item':item_cat3['itemid'][i],'recoms':{r.itemid:r.similarity for r in resultData[['itemid','similarity']].take(101)[0:100]}}\n",
    "                fil.write(str(pn).replace(\"'\",'\"')+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Дальше чёрная магия. Рисовал alexey.sotnikov (я не понял, как, но РАБОТАЕТ отлично!)\n",
    "# Данный блок объединяет строки с одинаковым 'item' (такое возникает из-за принадлежности одного товара\n",
    "# разным категориям), склеивая рекомендации, и оставляет сто одну самую похожую\n",
    "\n",
    "import json\n",
    "itemrecs={}\n",
    "with open('project02.txt','r') as fil:\n",
    "    for line in fil:\n",
    "        data = json.loads(line)\n",
    "        item = data['item']\n",
    "        tmrs = data['recoms']\n",
    "        if itemrecs.get(item) is None :\n",
    "            itemrecs[item] = tmrs\n",
    "        else :\n",
    "            for i in tmrs :\n",
    "                if itemrecs[item].get(i) is None :\n",
    "                    itemrecs[item][i] = tmrs[i]\n",
    "                elif itemrecs[item][i] < tmrs[i] :\n",
    "                    itemrecs[item][i] = tmrs[i]\n",
    "s=''\n",
    "with open('project02.001','w') as fil:\n",
    "    for items in itemrecs:\n",
    "        s = '{\"item\": \"' + str(items) +'\", \"recoms\": {'\n",
    "        sortedlist = sorted(itemrecs[items].items(), key = lambda x: float(x[1]), reverse = True)\n",
    "        if len(sortedlist)>101 :\n",
    "            sortedlist = sortedlist[0:101]\n",
    "        for i in range(len(sortedlist)):\n",
    "            if i == 0 :\n",
    "                delimiter = ''\n",
    "            else :\n",
    "                delimiter = ', '\n",
    "            s = s + delimiter + '\"' + sortedlist[i][0] + '\": ' + str(sortedlist[i][1])\n",
    "        s = s + '}}'\n",
    "        fil.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данный блок склеивает полученный файл с тестом по item, удаляет первую похожесть (на саму себя со значением 100000) \n",
    "# и добавляет к непосчитанным строкам похожесть - саму себя со значеним 99999\n",
    "\n",
    "itemrecs={}\n",
    "with open('project02.001','r') as fil:\n",
    "    for line in fil:\n",
    "        data = json.loads(line)\n",
    "        item = data['item']\n",
    "        tmrs = data['recoms']\n",
    "        if itemrecs.get(item) is None :\n",
    "            itemrecs[item] = tmrs\n",
    "        else :\n",
    "            for i in tmrs :\n",
    "                if itemrecs[item].get(i) is None :\n",
    "                    itemrecs[item][i] = tmrs[i]\n",
    "                elif itemrecs[item][i] < tmrs[i] :\n",
    "                    itemrecs[item][i] = tmrs[i]\n",
    "\n",
    "with open('project02.txt', 'w') as f_end:\n",
    "    with open('ozon_test.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            item = data['item']\n",
    "            recoms = {}\n",
    "            if itemrecs.get(item) is not None :\n",
    "                recoms = itemrecs[item]\n",
    "                recoms.pop(item, None)\n",
    "                sortedlist = sorted(recoms.items(), key = lambda x: float(x[1]), reverse = True)\n",
    "                s = '{\"item\": \"' + str(item) +'\", \"recoms\": {'\n",
    "                for i in range(len(sortedlist)):\n",
    "                    if i == 0 :\n",
    "                        delimiter = ''\n",
    "                    else :\n",
    "                        delimiter = ', '\n",
    "                    s = s + delimiter + '\"' + sortedlist[i][0] + '\": ' + str(sortedlist[i][1])\n",
    "                s = s + '}}'\n",
    "            else :\n",
    "                s = '{\"item\": \"' + str(item) +'\", \"recoms\": {\"' + str(item) + '\": 99999.99}}'\n",
    "            f_end.write(s+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # \"Не забудьте освободить ресурсы\" (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С чем пришлось столкнуться:\n",
    "#### 1) в лоб решать задачу нереально, предварительный прогноз расчета - больше месяца\n",
    "#### 2) нашлись три интересных элемента в тесте. У двух нет ссылки на каталог (но здесь можно хотя бы поискать похожести на всём списке товаров), а один вообще отсутствует в списке товаров. Ни описания, ничего :)\n",
    "#### 2) попытки понять точнее, как работает ozon, особым успехом не увенчались, так как каталог практически полностью обновился. Если 5 лет назад предлагали книги к каким-то товарам, то теперь это DVD, как минимум и т.д. Да и алгоритмы у них уже сильно поменялись. По товару из хака ни одна рекомендация на сайте не добавляет микробаллов :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
